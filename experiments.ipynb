{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/anshuljethani/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg\n",
    "import  pandas as pd\n",
    "\n",
    "data=gutenberg.raw('shakespeare-hamlet.txt')\n",
    "\n",
    "with open('hamlet.txt','w') as file:\n",
    "    file.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2618"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "with open('metamorphosis_clean.txt','r') as file:\n",
    "    text=file.read().lower()\n",
    "\n",
    "## Tokenize the text-creating indexes for words\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words=len(tokenizer.word_index)+1\n",
    "total_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'to': 2,\n",
       " 'and': 3,\n",
       " 'he': 4,\n",
       " 'his': 5,\n",
       " 'of': 6,\n",
       " 'was': 7,\n",
       " 'it': 8,\n",
       " 'had': 9,\n",
       " 'in': 10,\n",
       " 'that': 11,\n",
       " 'a': 12,\n",
       " 'as': 13,\n",
       " 'gregor': 14,\n",
       " 'with': 15,\n",
       " 'she': 16,\n",
       " 'him': 17,\n",
       " 'her': 18,\n",
       " 'would': 19,\n",
       " 'not': 20,\n",
       " 'but': 21,\n",
       " 'at': 22,\n",
       " 'for': 23,\n",
       " 'they': 24,\n",
       " 'on': 25,\n",
       " 'all': 26,\n",
       " 'room': 27,\n",
       " 'from': 28,\n",
       " 'could': 29,\n",
       " 'be': 30,\n",
       " 'out': 31,\n",
       " 'have': 32,\n",
       " 'if': 33,\n",
       " 'there': 34,\n",
       " 'been': 35,\n",
       " \"gregor's\": 36,\n",
       " 'so': 37,\n",
       " 'father': 38,\n",
       " 'sister': 39,\n",
       " 'this': 40,\n",
       " 'now': 41,\n",
       " 'himself': 42,\n",
       " 'door': 43,\n",
       " 'then': 44,\n",
       " 'back': 45,\n",
       " 'mother': 46,\n",
       " 'up': 47,\n",
       " 'even': 48,\n",
       " 'into': 49,\n",
       " 'no': 50,\n",
       " 'did': 51,\n",
       " 'more': 52,\n",
       " 'one': 53,\n",
       " 'their': 54,\n",
       " 'when': 55,\n",
       " 'were': 56,\n",
       " 'what': 57,\n",
       " 'about': 58,\n",
       " 'them': 59,\n",
       " 'way': 60,\n",
       " 'only': 61,\n",
       " 'time': 62,\n",
       " 'i': 63,\n",
       " 'by': 64,\n",
       " 'than': 65,\n",
       " 'you': 66,\n",
       " 'just': 67,\n",
       " 'said': 68,\n",
       " 'little': 69,\n",
       " 'any': 70,\n",
       " 'do': 71,\n",
       " 'get': 72,\n",
       " 'other': 73,\n",
       " 'still': 74,\n",
       " 'first': 75,\n",
       " 'or': 76,\n",
       " 'made': 77,\n",
       " 'go': 78,\n",
       " 'some': 79,\n",
       " 'while': 80,\n",
       " 'see': 81,\n",
       " 'again': 82,\n",
       " 'without': 83,\n",
       " 'like': 84,\n",
       " 'head': 85,\n",
       " 'before': 86,\n",
       " 'much': 87,\n",
       " 'after': 88,\n",
       " 'where': 89,\n",
       " 'chief': 90,\n",
       " 'down': 91,\n",
       " 'open': 92,\n",
       " 'samsa': 93,\n",
       " 'which': 94,\n",
       " 'very': 95,\n",
       " 'clerk': 96,\n",
       " 'who': 97,\n",
       " 'thought': 98,\n",
       " 'well': 99,\n",
       " 'over': 100,\n",
       " 'went': 101,\n",
       " 'come': 102,\n",
       " 'away': 103,\n",
       " 'left': 104,\n",
       " 'an': 105,\n",
       " 'soon': 106,\n",
       " 'family': 107,\n",
       " 'came': 108,\n",
       " 'too': 109,\n",
       " 'quite': 110,\n",
       " 'though': 111,\n",
       " 'bed': 112,\n",
       " 'looked': 113,\n",
       " 'how': 114,\n",
       " 'day': 115,\n",
       " 'everything': 116,\n",
       " 'wanted': 117,\n",
       " 'something': 118,\n",
       " 'let': 119,\n",
       " 'against': 120,\n",
       " 'two': 121,\n",
       " 'seemed': 122,\n",
       " 'is': 123,\n",
       " 'being': 124,\n",
       " 'work': 125,\n",
       " 'we': 126,\n",
       " 'parents': 127,\n",
       " 'round': 128,\n",
       " 'three': 129,\n",
       " 'gentlemen': 130,\n",
       " 'body': 131,\n",
       " 'things': 132,\n",
       " 'able': 133,\n",
       " 'because': 134,\n",
       " 'make': 135,\n",
       " 'already': 136,\n",
       " 'really': 137,\n",
       " 'mr': 138,\n",
       " 'morning': 139,\n",
       " 'look': 140,\n",
       " 'slowly': 141,\n",
       " 'long': 142,\n",
       " 'side': 143,\n",
       " 'floor': 144,\n",
       " 'hand': 145,\n",
       " 'grete': 146,\n",
       " 'onto': 147,\n",
       " 'got': 148,\n",
       " 'my': 149,\n",
       " 'legs': 150,\n",
       " 'used': 151,\n",
       " 'never': 152,\n",
       " 'better': 153,\n",
       " 'enough': 154,\n",
       " 'think': 155,\n",
       " 'through': 156,\n",
       " 'flat': 157,\n",
       " 'around': 158,\n",
       " 'lay': 159,\n",
       " 'off': 160,\n",
       " 'although': 161,\n",
       " 'table': 162,\n",
       " 'turned': 163,\n",
       " 'window': 164,\n",
       " 'heard': 165,\n",
       " 'feel': 166,\n",
       " 'right': 167,\n",
       " 'eyes': 168,\n",
       " 'home': 169,\n",
       " 'become': 170,\n",
       " 'hands': 171,\n",
       " 'under': 172,\n",
       " 'here': 173,\n",
       " 'good': 174,\n",
       " 'its': 175,\n",
       " 'whole': 176,\n",
       " 'longer': 177,\n",
       " 'always': 178,\n",
       " 'must': 179,\n",
       " 'began': 180,\n",
       " 'once': 181,\n",
       " 'possible': 182,\n",
       " 'next': 183,\n",
       " 'perhaps': 184,\n",
       " 'nothing': 185,\n",
       " 'almost': 186,\n",
       " 'took': 187,\n",
       " 'say': 188,\n",
       " 'evening': 189,\n",
       " 'straight': 190,\n",
       " 'living': 191,\n",
       " 'herself': 192,\n",
       " 'me': 193,\n",
       " 'know': 194,\n",
       " 'should': 195,\n",
       " 'voice': 196,\n",
       " 'each': 197,\n",
       " 'might': 198,\n",
       " 'front': 199,\n",
       " 'couch': 200,\n",
       " 'hardly': 201,\n",
       " 'your': 202,\n",
       " \"it's\": 203,\n",
       " 'move': 204,\n",
       " 'same': 205,\n",
       " 'help': 206,\n",
       " 'opened': 207,\n",
       " 'immediately': 208,\n",
       " 'asked': 209,\n",
       " 'chair': 210,\n",
       " 'behind': 211,\n",
       " 'happened': 212,\n",
       " 'times': 213,\n",
       " 'felt': 214,\n",
       " 'business': 215,\n",
       " 'can': 216,\n",
       " 'furniture': 217,\n",
       " 'probably': 218,\n",
       " \"i'm\": 219,\n",
       " 'often': 220,\n",
       " 'became': 221,\n",
       " 'course': 222,\n",
       " 'also': 223,\n",
       " 'women': 224,\n",
       " 'taken': 225,\n",
       " 'sat': 226,\n",
       " 'sleep': 227,\n",
       " 'doing': 228,\n",
       " 'food': 229,\n",
       " 'these': 230,\n",
       " 'chest': 231,\n",
       " 'put': 232,\n",
       " 'called': 233,\n",
       " 'anything': 234,\n",
       " 'arms': 235,\n",
       " 'held': 236,\n",
       " 'stood': 237,\n",
       " 'attention': 238,\n",
       " 'pressed': 239,\n",
       " 'face': 240,\n",
       " 'found': 241,\n",
       " 'life': 242,\n",
       " 'close': 243,\n",
       " 'money': 244,\n",
       " 'certainly': 245,\n",
       " 'seen': 246,\n",
       " 'hear': 247,\n",
       " 'until': 248,\n",
       " 'playing': 249,\n",
       " 'towards': 250,\n",
       " 'hard': 251,\n",
       " 'tried': 252,\n",
       " 'pushed': 253,\n",
       " 'saw': 254,\n",
       " 'quickly': 255,\n",
       " 'especially': 256,\n",
       " 'later': 257,\n",
       " 'ever': 258,\n",
       " 'words': 259,\n",
       " 'done': 260,\n",
       " \"don't\": 261,\n",
       " 'forward': 262,\n",
       " 'new': 263,\n",
       " 'every': 264,\n",
       " 'leave': 265,\n",
       " 'reason': 266,\n",
       " 'understand': 267,\n",
       " 'take': 268,\n",
       " 'old': 269,\n",
       " 'violin': 270,\n",
       " 'pain': 271,\n",
       " 'making': 272,\n",
       " \"didn't\": 273,\n",
       " 'thing': 274,\n",
       " \"that's\": 275,\n",
       " 'moving': 276,\n",
       " 'set': 277,\n",
       " 'want': 278,\n",
       " 'give': 279,\n",
       " 'most': 280,\n",
       " 'keep': 281,\n",
       " 'last': 282,\n",
       " 'stay': 283,\n",
       " 'moved': 284,\n",
       " 'remained': 285,\n",
       " 'least': 286,\n",
       " 'us': 287,\n",
       " 'kitchen': 288,\n",
       " 'clear': 289,\n",
       " 'ran': 290,\n",
       " 'key': 291,\n",
       " 'eat': 292,\n",
       " 'middle': 293,\n",
       " 'moment': 294,\n",
       " \"what's\": 295,\n",
       " 'state': 296,\n",
       " 'however': 297,\n",
       " 'effort': 298,\n",
       " 'own': 299,\n",
       " 'whether': 300,\n",
       " 'doors': 301,\n",
       " 'night': 302,\n",
       " 'themselves': 303,\n",
       " 'finally': 304,\n",
       " 'despite': 305,\n",
       " 'will': 306,\n",
       " 'quiet': 307,\n",
       " 'mrs': 308,\n",
       " 'few': 309,\n",
       " 'great': 310,\n",
       " 'turn': 311,\n",
       " 'looking': 312,\n",
       " 'upright': 313,\n",
       " 'covered': 314,\n",
       " 'present': 315,\n",
       " 'shut': 316,\n",
       " 'stopped': 317,\n",
       " 'train': 318,\n",
       " 'boss': 319,\n",
       " 'told': 320,\n",
       " 'desk': 321,\n",
       " 'together': 322,\n",
       " 'another': 323,\n",
       " 'drawers': 324,\n",
       " 'half': 325,\n",
       " 'seven': 326,\n",
       " 'completely': 327,\n",
       " 'noticed': 328,\n",
       " 'carefully': 329,\n",
       " 'dressed': 330,\n",
       " 'fell': 331,\n",
       " \"can't\": 332,\n",
       " 'clearly': 333,\n",
       " 'use': 334,\n",
       " 'far': 335,\n",
       " 'why': 336,\n",
       " 'suddenly': 337,\n",
       " 'since': 338,\n",
       " 'tears': 339,\n",
       " 'realised': 340,\n",
       " 'taking': 341,\n",
       " 'going': 342,\n",
       " 'charwoman': 343,\n",
       " 'cleaner': 344,\n",
       " 'slightly': 345,\n",
       " 'travelling': 346,\n",
       " \"couldn't\": 347,\n",
       " 'position': 348,\n",
       " 'anyone': 349,\n",
       " 'place': 350,\n",
       " 'eating': 351,\n",
       " 'notice': 352,\n",
       " 'fall': 353,\n",
       " \"i'll\": 354,\n",
       " 'yes': 355,\n",
       " 'noise': 356,\n",
       " 'having': 357,\n",
       " 'word': 358,\n",
       " 'peace': 359,\n",
       " 'bring': 360,\n",
       " 'sign': 361,\n",
       " 'others': 362,\n",
       " 'force': 363,\n",
       " 'kept': 364,\n",
       " 'loud': 365,\n",
       " 'across': 366,\n",
       " 'needed': 367,\n",
       " 'knew': 368,\n",
       " 'hours': 369,\n",
       " 'speak': 370,\n",
       " 'please': 371,\n",
       " 'else': 372,\n",
       " 'nearly': 373,\n",
       " \"he's\": 374,\n",
       " 'job': 375,\n",
       " 'condition': 376,\n",
       " 'hurried': 377,\n",
       " 'lock': 378,\n",
       " 'mouth': 379,\n",
       " 'leant': 380,\n",
       " 'several': 381,\n",
       " 'uniform': 382,\n",
       " 'finished': 383,\n",
       " 'doorway': 384,\n",
       " 'closed': 385,\n",
       " 'everyone': 386,\n",
       " 'stand': 387,\n",
       " 'rest': 388,\n",
       " 'between': 389,\n",
       " 'walls': 390,\n",
       " 'spread': 391,\n",
       " 'nice': 392,\n",
       " 'lower': 393,\n",
       " 'different': 394,\n",
       " 'white': 395,\n",
       " 'getting': 396,\n",
       " 'during': 397,\n",
       " 'maybe': 398,\n",
       " 'years': 399,\n",
       " \"o'clock\": 400,\n",
       " 'man': 401,\n",
       " 'ill': 402,\n",
       " 'hurriedly': 403,\n",
       " 'near': 404,\n",
       " 'full': 405,\n",
       " 'conversation': 406,\n",
       " 'need': 407,\n",
       " 'both': 408,\n",
       " 'breakfast': 409,\n",
       " 'instead': 410,\n",
       " 'serious': 411,\n",
       " 'difficult': 412,\n",
       " 'part': 413,\n",
       " 'turning': 414,\n",
       " 'carry': 415,\n",
       " 'earlier': 416,\n",
       " 'order': 417,\n",
       " 'calm': 418,\n",
       " 'expected': 419,\n",
       " 'maid': 420,\n",
       " 'sound': 421,\n",
       " 'question': 422,\n",
       " 'simply': 423,\n",
       " 'easy': 424,\n",
       " 'strength': 425,\n",
       " 'knowing': 426,\n",
       " 'sight': 427,\n",
       " \"mother's\": 428,\n",
       " 'hall': 429,\n",
       " 'kind': 430,\n",
       " 'breath': 431,\n",
       " 'ground': 432,\n",
       " 'chance': 433,\n",
       " 'reached': 434,\n",
       " 'pleasure': 435,\n",
       " 'end': 436,\n",
       " 'fully': 437,\n",
       " 'light': 438,\n",
       " 'ceiling': 439,\n",
       " 'actually': 440,\n",
       " 'dish': 441,\n",
       " 'sometimes': 442,\n",
       " 'music': 443,\n",
       " 'totally': 444,\n",
       " 'tired': 445,\n",
       " 'crawl': 446,\n",
       " 'lifted': 447,\n",
       " 'picture': 448,\n",
       " 'forget': 449,\n",
       " 'unable': 450,\n",
       " 'sleeping': 451,\n",
       " 'oh': 452,\n",
       " 'god': 453,\n",
       " 'people': 454,\n",
       " 'early': 455,\n",
       " 'whenever': 456,\n",
       " 'are': 457,\n",
       " 'try': 458,\n",
       " 'given': 459,\n",
       " 'tell': 460,\n",
       " 'sort': 461,\n",
       " 'five': 462,\n",
       " 'alarm': 463,\n",
       " 'past': 464,\n",
       " 'quarter': 465,\n",
       " 'fresh': 466,\n",
       " 'office': 467,\n",
       " 'doctor': 468,\n",
       " 'wrong': 469,\n",
       " 'struck': 470,\n",
       " 'leaving': 471,\n",
       " 'aware': 472,\n",
       " 'habit': 473,\n",
       " 'lying': 474,\n",
       " 'caused': 475,\n",
       " 'slightest': 476,\n",
       " 'free': 477,\n",
       " 'painfully': 478,\n",
       " 'air': 479,\n",
       " 'unfortunately': 480,\n",
       " 'street': 481,\n",
       " 'carpet': 482,\n",
       " 'find': 483,\n",
       " 'someone': 484,\n",
       " 'anyway': 485,\n",
       " 'sure': 486,\n",
       " \"you're\": 487,\n",
       " 'year': 488,\n",
       " 'gave': 489,\n",
       " 'listen': 490,\n",
       " 'listening': 491,\n",
       " 'hold': 492,\n",
       " 'meal': 493,\n",
       " 'landing': 494,\n",
       " 'earn': 495,\n",
       " 'impossible': 496,\n",
       " 'foot': 497,\n",
       " 'rushed': 498,\n",
       " 'lost': 499,\n",
       " 'forgotten': 500,\n",
       " 'running': 501,\n",
       " 'flew': 502,\n",
       " \"father's\": 503,\n",
       " 'milk': 504,\n",
       " 'crawling': 505,\n",
       " 'days': 506,\n",
       " 'bent': 507,\n",
       " 'sheet': 508,\n",
       " 'meant': 509,\n",
       " 'many': 510,\n",
       " \"wasn't\": 511,\n",
       " 'human': 512,\n",
       " 'cut': 513,\n",
       " 'threw': 514,\n",
       " \"wouldn't\": 515,\n",
       " 'friendly': 516,\n",
       " 'overcome': 517,\n",
       " \"i'd\": 518,\n",
       " 'gone': 519,\n",
       " 'pay': 520,\n",
       " 'clock': 521,\n",
       " 'quietly': 522,\n",
       " 'rush': 523,\n",
       " 'fact': 524,\n",
       " 'usual': 525,\n",
       " 'shocked': 526,\n",
       " 'outside': 527,\n",
       " 'short': 528,\n",
       " 'sides': 529,\n",
       " 'throw': 530,\n",
       " 'learned': 531,\n",
       " 'easily': 532,\n",
       " 'followed': 533,\n",
       " 'harder': 534,\n",
       " 'consideration': 535,\n",
       " 'raised': 536,\n",
       " 'happen': 537,\n",
       " 'patient': 538,\n",
       " 'locked': 539,\n",
       " 'steps': 540,\n",
       " 'necessary': 541,\n",
       " 'show': 542,\n",
       " 'stayed': 543,\n",
       " 'idea': 544,\n",
       " 'glad': 545,\n",
       " 'seriously': 546,\n",
       " 'speaking': 547,\n",
       " 'late': 548,\n",
       " 'beside': 549,\n",
       " 'alright': 550,\n",
       " 'wait': 551,\n",
       " 'loudly': 552,\n",
       " 'holding': 553,\n",
       " 'using': 554,\n",
       " 'start': 555,\n",
       " 'disappeared': 556,\n",
       " 'along': 557,\n",
       " 'wall': 558,\n",
       " 'enormous': 559,\n",
       " 'urge': 560,\n",
       " 'spare': 561,\n",
       " 'stick': 562,\n",
       " 'pulled': 563,\n",
       " 'stop': 564,\n",
       " 'allowed': 565,\n",
       " 'intentions': 566,\n",
       " 'slammed': 567,\n",
       " 'filled': 568,\n",
       " 'either': 569,\n",
       " 'forced': 570,\n",
       " \"sister's\": 571,\n",
       " 'surprise': 572,\n",
       " 'carried': 573,\n",
       " 'startled': 574,\n",
       " 'asleep': 575,\n",
       " 'broom': 576,\n",
       " 'send': 577,\n",
       " 'indeed': 578,\n",
       " 'silent': 579,\n",
       " 'slow': 580,\n",
       " 'coming': 581,\n",
       " 'writing': 582,\n",
       " 'immobile': 583,\n",
       " 'shouted': 584,\n",
       " 'apple': 585,\n",
       " 'sit': 586,\n",
       " 'appeared': 587,\n",
       " 'gentleman': 588,\n",
       " 'brown': 589,\n",
       " 'ready': 590,\n",
       " 'small': 591,\n",
       " 'four': 592,\n",
       " 'above': 593,\n",
       " 'hung': 594,\n",
       " 'lady': 595,\n",
       " 'fur': 596,\n",
       " 'hat': 597,\n",
       " 'raising': 598,\n",
       " 'heavy': 599,\n",
       " 'arm': 600,\n",
       " 'bit': 601,\n",
       " \"i've\": 602,\n",
       " \"there's\": 603,\n",
       " 'slight': 604,\n",
       " 'drew': 605,\n",
       " 'touched': 606,\n",
       " 'live': 607,\n",
       " 'ought': 608,\n",
       " 'best': 609,\n",
       " 'talking': 610,\n",
       " 'hope': 611,\n",
       " 'change': 612,\n",
       " 'forwards': 613,\n",
       " 'catch': 614,\n",
       " 'entirely': 615,\n",
       " 'somebody': 616,\n",
       " 'somewhere': 617,\n",
       " 'answer': 618,\n",
       " 'saying': 619,\n",
       " 'gently': 620,\n",
       " 'answered': 621,\n",
       " 'remove': 622,\n",
       " 'disturbed': 623,\n",
       " 'thoughts': 624,\n",
       " 'sensible': 625,\n",
       " 'today': 626,\n",
       " 'push': 627,\n",
       " 'those': 628,\n",
       " 'control': 629,\n",
       " 'itself': 630,\n",
       " 'trying': 631,\n",
       " 'direction': 632,\n",
       " 'hit': 633,\n",
       " 'pushing': 634,\n",
       " 'whatever': 635,\n",
       " 'view': 636,\n",
       " 'confidence': 637,\n",
       " 'entire': 638,\n",
       " 'concern': 639,\n",
       " 'strong': 640,\n",
       " 'mind': 641,\n",
       " 'careful': 642,\n",
       " 'call': 643,\n",
       " 'difficulty': 644,\n",
       " 'smile': 645,\n",
       " 'decision': 646,\n",
       " 'highly': 647,\n",
       " 'employees': 648,\n",
       " 'annoyed': 649,\n",
       " 'fallen': 650,\n",
       " 'has': 651,\n",
       " \"isn't\": 652,\n",
       " 'evenings': 653,\n",
       " 'working': 654,\n",
       " 'amazed': 655,\n",
       " 'worry': 656,\n",
       " 'happening': 657,\n",
       " 'astonished': 658,\n",
       " 'seem': 659,\n",
       " 'animal': 660,\n",
       " 'entrance': 661,\n",
       " 'locksmith': 662,\n",
       " 'skirts': 663,\n",
       " 'situation': 664,\n",
       " 'teeth': 665,\n",
       " 'lack': 666,\n",
       " 'break': 667,\n",
       " 'wide': 668,\n",
       " 'movement': 669,\n",
       " 'important': 670,\n",
       " 'reading': 671,\n",
       " 'exactly': 672,\n",
       " 'respect': 673,\n",
       " 'our': 674,\n",
       " 'nobody': 675,\n",
       " 'started': 676,\n",
       " 'stretched': 677,\n",
       " 'save': 678,\n",
       " 'mood': 679,\n",
       " 'future': 680,\n",
       " 'talk': 681,\n",
       " 'backwards': 682,\n",
       " 'self': 683,\n",
       " 'newspaper': 684,\n",
       " 'drive': 685,\n",
       " 'merely': 686,\n",
       " 'tip': 687,\n",
       " 'obviously': 688,\n",
       " 'dark': 689,\n",
       " 'crawled': 690,\n",
       " 'darkness': 691,\n",
       " 'such': 692,\n",
       " 'waited': 693,\n",
       " 'empty': 694,\n",
       " 'nonetheless': 695,\n",
       " 'spent': 696,\n",
       " 'bear': 697,\n",
       " 'anxiously': 698,\n",
       " 'rather': 699,\n",
       " 'feet': 700,\n",
       " 'brought': 701,\n",
       " 'month': 702,\n",
       " 'breathe': 703,\n",
       " 'becoming': 704,\n",
       " 'dinner': 705,\n",
       " 'fetch': 706,\n",
       " 'decided': 707,\n",
       " 'appearance': 708,\n",
       " 'high': 709,\n",
       " 'stepped': 710,\n",
       " 'broke': 711,\n",
       " 'arrived': 712,\n",
       " 'coat': 713,\n",
       " 'pockets': 714,\n",
       " 'friends': 715,\n",
       " 'bedroom': 716,\n",
       " 'briefly': 717,\n",
       " 'corner': 718,\n",
       " 'rented': 719,\n",
       " 'rid': 720,\n",
       " 'horrible': 721,\n",
       " 'belly': 722,\n",
       " 'stiff': 723,\n",
       " 'proper': 724,\n",
       " 'peacefully': 725,\n",
       " 'samples': 726,\n",
       " 'recently': 727,\n",
       " 'showed': 728,\n",
       " 'rain': 729,\n",
       " 'sad': 730,\n",
       " 'rolled': 731,\n",
       " 'top': 732,\n",
       " 'bad': 733,\n",
       " 'lift': 734,\n",
       " 'cold': 735,\n",
       " 'slid': 736,\n",
       " 'salesmen': 737,\n",
       " 'sitting': 738,\n",
       " 'spot': 739,\n",
       " 'ago': 740,\n",
       " 'hearing': 741,\n",
       " 'debt': 742,\n",
       " 'six': 743,\n",
       " 'big': 744,\n",
       " 'true': 745,\n",
       " 'deeply': 746,\n",
       " 'lively': 747,\n",
       " 'avoid': 748,\n",
       " 'anger': 749,\n",
       " 'report': 750,\n",
       " 'understanding': 751,\n",
       " 'extremely': 752,\n",
       " 'suspicious': 753,\n",
       " 'yet': 754,\n",
       " 'company': 755,\n",
       " 'apart': 756,\n",
       " 'knock': 757,\n",
       " 'deep': 758,\n",
       " 'inside': 759,\n",
       " 'mixed': 760,\n",
       " 'properly': 761,\n",
       " 'explanation': 762,\n",
       " 'knocking': 763,\n",
       " 'opening': 764,\n",
       " 'consider': 765,\n",
       " 'remembered': 766,\n",
       " 'simple': 767,\n",
       " 'broad': 768,\n",
       " 'directions': 769,\n",
       " 'stretch': 770,\n",
       " 'managed': 771,\n",
       " 'weight': 772,\n",
       " 'occurred': 773,\n",
       " 'injured': 774,\n",
       " 'afraid': 775,\n",
       " 'watching': 776,\n",
       " 'bringing': 777,\n",
       " 'narrow': 778,\n",
       " 'lightly': 779,\n",
       " 'total': 780,\n",
       " 'ask': 781,\n",
       " 'falling': 782,\n",
       " 'raise': 783,\n",
       " 'forth': 784,\n",
       " 'caught': 785,\n",
       " 'firm': 786,\n",
       " 'spend': 787,\n",
       " 'upset': 788,\n",
       " \"clerk's\": 789,\n",
       " 'wants': 790,\n",
       " \"he'll\": 791,\n",
       " 'town': 792,\n",
       " \"you'll\": 793,\n",
       " 'explaining': 794,\n",
       " 'silence': 795,\n",
       " 'crying': 796,\n",
       " 'intention': 797,\n",
       " 'suitable': 798,\n",
       " 'behaviour': 799,\n",
       " 'causing': 800,\n",
       " 'employer': 801,\n",
       " 'immediate': 802,\n",
       " 'am': 803,\n",
       " 'giving': 804,\n",
       " 'wish': 805,\n",
       " 'nor': 806,\n",
       " 'waste': 807,\n",
       " 'learn': 808,\n",
       " 'sir': 809,\n",
       " 'shocking': 810,\n",
       " 'suffer': 811,\n",
       " 'sent': 812,\n",
       " 'calmly': 813,\n",
       " \"we're\": 814,\n",
       " 'contrast': 815,\n",
       " 'drawn': 816,\n",
       " 'meanwhile': 817,\n",
       " 'flowed': 818,\n",
       " 'following': 819,\n",
       " 'efforts': 820,\n",
       " 'double': 821,\n",
       " 'occupied': 822,\n",
       " 'hair': 823,\n",
       " 'grey': 824,\n",
       " 'line': 825,\n",
       " 'large': 826,\n",
       " 'remember': 827,\n",
       " 'usually': 828,\n",
       " 'partly': 829,\n",
       " 'shoulders': 830,\n",
       " 'secret': 831,\n",
       " 'sudden': 832,\n",
       " 'waiting': 833,\n",
       " 'convinced': 834,\n",
       " 'provide': 835,\n",
       " 'persuade': 836,\n",
       " 'understood': 837,\n",
       " 'landed': 838,\n",
       " 'outstretched': 839,\n",
       " 'sake': 840,\n",
       " 'coffee': 841,\n",
       " 'fled': 842,\n",
       " 'relatively': 843,\n",
       " 'picked': 844,\n",
       " 'appeals': 845,\n",
       " 'hissing': 846,\n",
       " 'distance': 847,\n",
       " 'further': 848,\n",
       " 'fast': 849,\n",
       " 'shove': 850,\n",
       " 'heavily': 851,\n",
       " 'feeling': 852,\n",
       " 'smell': 853,\n",
       " 'normally': 854,\n",
       " 'enter': 855,\n",
       " 'toe': 856,\n",
       " 'remain': 857,\n",
       " 'uneasy': 858,\n",
       " 'regret': 859,\n",
       " 'hunger': 860,\n",
       " 'greatest': 861,\n",
       " 'test': 862,\n",
       " 'watched': 863,\n",
       " 'realise': 864,\n",
       " 'bare': 865,\n",
       " 'cheese': 866,\n",
       " 'comfortable': 867,\n",
       " 'liked': 868,\n",
       " 'finger': 869,\n",
       " 'eaten': 870,\n",
       " 'midday': 871,\n",
       " 'news': 872,\n",
       " 'delay': 873,\n",
       " 'ate': 874,\n",
       " 'misfortune': 875,\n",
       " 'despair': 876,\n",
       " 'success': 877,\n",
       " 'earned': 878,\n",
       " 'warm': 879,\n",
       " 'fond': 880,\n",
       " 'conservatory': 881,\n",
       " 'christmas': 882,\n",
       " 'continue': 883,\n",
       " 'pull': 884,\n",
       " 'repeated': 885,\n",
       " 'lot': 886,\n",
       " 'interest': 887,\n",
       " 'nodded': 888,\n",
       " 'closer': 889,\n",
       " 'strain': 890,\n",
       " 'sofa': 891,\n",
       " 'child': 892,\n",
       " 'clothes': 893,\n",
       " 'experienced': 894,\n",
       " 'easier': 895,\n",
       " 'carrying': 896,\n",
       " 'improvement': 897,\n",
       " 'letting': 898,\n",
       " 'entertaining': 899,\n",
       " 'dare': 900,\n",
       " 'thrown': 901,\n",
       " \"we've\": 902,\n",
       " 'insist': 903,\n",
       " 'admit': 904,\n",
       " 'worn': 905,\n",
       " 'changed': 906,\n",
       " \"let's\": 907,\n",
       " 'chase': 908,\n",
       " \"grete's\": 909,\n",
       " 'screamed': 910,\n",
       " 'shaking': 911,\n",
       " 'death': 912,\n",
       " 'mean': 913,\n",
       " 'standing': 914,\n",
       " 'gold': 915,\n",
       " 'buttons': 916,\n",
       " 'cap': 917,\n",
       " 'movements': 918,\n",
       " 'contrary': 919,\n",
       " 'absolutely': 920,\n",
       " 'dust': 921,\n",
       " 'clean': 922,\n",
       " 'chairs': 923,\n",
       " 'play': 924,\n",
       " 'beards': 925,\n",
       " 'hurry': 926,\n",
       " 'smiled': 927,\n",
       " 'played': 928,\n",
       " 'hallway': 929,\n",
       " 'young': 930,\n",
       " 'neck': 931,\n",
       " 'dead': 932,\n",
       " 'daughter': 933,\n",
       " 'letters': 934,\n",
       " 'woke': 935,\n",
       " 'dreams': 936,\n",
       " 'thin': 937,\n",
       " 'size': 938,\n",
       " 'dream': 939,\n",
       " 'familiar': 940,\n",
       " 'collection': 941,\n",
       " 'salesman': 942,\n",
       " 'frame': 943,\n",
       " 'fitted': 944,\n",
       " 'dull': 945,\n",
       " 'weather': 946,\n",
       " 'drops': 947,\n",
       " 'hitting': 948,\n",
       " 'pane': 949,\n",
       " 'chosen': 950,\n",
       " 'curse': 951,\n",
       " 'worries': 952,\n",
       " 'itch': 953,\n",
       " 'makes': 954,\n",
       " 'stupid': 955,\n",
       " \"you've\": 956,\n",
       " 'instance': 957,\n",
       " 'house': 958,\n",
       " 'breakfasts': 959,\n",
       " \"he'd\": 960,\n",
       " 'definitely': 961,\n",
       " 'rung': 962,\n",
       " 'slept': 963,\n",
       " 'mad': 964,\n",
       " 'particularly': 965,\n",
       " \"boss's\": 966,\n",
       " 'assistant': 967,\n",
       " 'fifteen': 968,\n",
       " 'service': 969,\n",
       " 'accuse': 970,\n",
       " 'son': 971,\n",
       " 'hungrier': 972,\n",
       " 'cautious': 973,\n",
       " 'painful': 974,\n",
       " 'explain': 975,\n",
       " 'circumstances': 976,\n",
       " 'thank': 977,\n",
       " 'wooden': 978,\n",
       " 'members': 979,\n",
       " 'fist': 980,\n",
       " \"aren't\": 981,\n",
       " 'putting': 982,\n",
       " 'individual': 983,\n",
       " 'whispered': 984,\n",
       " 'beg': 985,\n",
       " 'acquired': 986,\n",
       " 'conclusions': 987,\n",
       " 'matter': 988,\n",
       " 'covers': 989,\n",
       " 'blow': 990,\n",
       " 'moreover': 991,\n",
       " 'bend': 992,\n",
       " 'imagine': 993,\n",
       " 'gather': 994,\n",
       " 'sensitive': 995,\n",
       " 'eventually': 996,\n",
       " 'miracle': 997,\n",
       " 'price': 998,\n",
       " 'sacrifice': 999,\n",
       " 'remind': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating input-output sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences=[]\n",
    "for line in text.split('\\n'):\n",
    "    token_list=tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1,len(token_list)):\n",
    "        n_gram_sequence=token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1313, 139],\n",
       " [1313, 139, 55],\n",
       " [1313, 139, 55, 14],\n",
       " [1313, 139, 55, 14, 93],\n",
       " [1313, 139, 55, 14, 93, 935],\n",
       " [1313, 139, 55, 14, 93, 935, 28],\n",
       " [1313, 139, 55, 14, 93, 935, 28, 1314],\n",
       " [1313, 139, 55, 14, 93, 935, 28, 1314, 936],\n",
       " [1313, 139, 55, 14, 93, 935, 28, 1314, 936, 4],\n",
       " [1313, 139, 55, 14, 93, 935, 28, 1314, 936, 4, 241],\n",
       " [42, 1315],\n",
       " [42, 1315, 10],\n",
       " [42, 1315, 10, 5],\n",
       " [42, 1315, 10, 5, 112],\n",
       " [42, 1315, 10, 5, 112, 49],\n",
       " [42, 1315, 10, 5, 112, 49, 12],\n",
       " [42, 1315, 10, 5, 112, 49, 12, 721],\n",
       " [42, 1315, 10, 5, 112, 49, 12, 721, 1316],\n",
       " [42, 1315, 10, 5, 112, 49, 12, 721, 1316, 4],\n",
       " [42, 1315, 10, 5, 112, 49, 12, 721, 1316, 4, 159],\n",
       " [42, 1315, 10, 5, 112, 49, 12, 721, 1316, 4, 159, 25],\n",
       " [5, 1317],\n",
       " [5, 1317, 84],\n",
       " [5, 1317, 84, 45],\n",
       " [5, 1317, 84, 45, 3],\n",
       " [5, 1317, 84, 45, 3, 33],\n",
       " [5, 1317, 84, 45, 3, 33, 4],\n",
       " [5, 1317, 84, 45, 3, 33, 4, 447],\n",
       " [5, 1317, 84, 45, 3, 33, 4, 447, 5],\n",
       " [5, 1317, 84, 45, 3, 33, 4, 447, 5, 85],\n",
       " [5, 1317, 84, 45, 3, 33, 4, 447, 5, 85, 12],\n",
       " [5, 1317, 84, 45, 3, 33, 4, 447, 5, 85, 12, 69],\n",
       " [5, 1317, 84, 45, 3, 33, 4, 447, 5, 85, 12, 69, 4],\n",
       " [5, 1317, 84, 45, 3, 33, 4, 447, 5, 85, 12, 69, 4, 29],\n",
       " [81, 5],\n",
       " [81, 5, 589],\n",
       " [81, 5, 589, 722],\n",
       " [81, 5, 589, 722, 345],\n",
       " [81, 5, 589, 722, 345, 1318],\n",
       " [81, 5, 589, 722, 345, 1318, 3],\n",
       " [81, 5, 589, 722, 345, 1318, 3, 1319],\n",
       " [81, 5, 589, 722, 345, 1318, 3, 1319, 64],\n",
       " [81, 5, 589, 722, 345, 1318, 3, 1319, 64, 1320],\n",
       " [81, 5, 589, 722, 345, 1318, 3, 1319, 64, 1320, 49],\n",
       " [81, 5, 589, 722, 345, 1318, 3, 1319, 64, 1320, 49, 723],\n",
       " [1321, 1],\n",
       " [1321, 1, 1322],\n",
       " [1321, 1, 1322, 7],\n",
       " [1321, 1, 1322, 7, 201],\n",
       " [1321, 1, 1322, 7, 201, 133],\n",
       " [1321, 1, 1322, 7, 201, 133, 2],\n",
       " [1321, 1, 1322, 7, 201, 133, 2, 1323],\n",
       " [1321, 1, 1322, 7, 201, 133, 2, 1323, 8],\n",
       " [1321, 1, 1322, 7, 201, 133, 2, 1323, 8, 3],\n",
       " [1321, 1, 1322, 7, 201, 133, 2, 1323, 8, 3, 122],\n",
       " [1321, 1, 1322, 7, 201, 133, 2, 1323, 8, 3, 122, 590],\n",
       " [2, 1324],\n",
       " [2, 1324, 160],\n",
       " [2, 1324, 160, 70],\n",
       " [2, 1324, 160, 70, 294],\n",
       " [2, 1324, 160, 70, 294, 5],\n",
       " [2, 1324, 160, 70, 294, 5, 510],\n",
       " [2, 1324, 160, 70, 294, 5, 510, 150],\n",
       " [2, 1324, 160, 70, 294, 5, 510, 150, 1325],\n",
       " [2, 1324, 160, 70, 294, 5, 510, 150, 1325, 937],\n",
       " [2, 1324, 160, 70, 294, 5, 510, 150, 1325, 937, 1326],\n",
       " [15, 1],\n",
       " [15, 1, 938],\n",
       " [15, 1, 938, 6],\n",
       " [15, 1, 938, 6, 1],\n",
       " [15, 1, 938, 6, 1, 388],\n",
       " [15, 1, 938, 6, 1, 388, 6],\n",
       " [15, 1, 938, 6, 1, 388, 6, 17],\n",
       " [15, 1, 938, 6, 1, 388, 6, 17, 1327],\n",
       " [15, 1, 938, 6, 1, 388, 6, 17, 1327, 58],\n",
       " [15, 1, 938, 6, 1, 388, 6, 17, 1327, 58, 1328],\n",
       " [15, 1, 938, 6, 1, 388, 6, 17, 1327, 58, 1328, 13],\n",
       " [15, 1, 938, 6, 1, 388, 6, 17, 1327, 58, 1328, 13, 4],\n",
       " [295, 212],\n",
       " [295, 212, 2],\n",
       " [295, 212, 2, 193],\n",
       " [295, 212, 2, 193, 4],\n",
       " [295, 212, 2, 193, 4, 98],\n",
       " [295, 212, 2, 193, 4, 98, 8],\n",
       " [295, 212, 2, 193, 4, 98, 8, 511],\n",
       " [295, 212, 2, 193, 4, 98, 8, 511, 12],\n",
       " [295, 212, 2, 193, 4, 98, 8, 511, 12, 939],\n",
       " [295, 212, 2, 193, 4, 98, 8, 511, 12, 939, 5],\n",
       " [295, 212, 2, 193, 4, 98, 8, 511, 12, 939, 5, 27],\n",
       " [12, 724],\n",
       " [12, 724, 512],\n",
       " [12, 724, 512, 27],\n",
       " [12, 724, 512, 27, 161],\n",
       " [12, 724, 512, 27, 161, 12],\n",
       " [12, 724, 512, 27, 161, 12, 69],\n",
       " [12, 724, 512, 27, 161, 12, 69, 109],\n",
       " [12, 724, 512, 27, 161, 12, 69, 109, 591],\n",
       " [12, 724, 512, 27, 161, 12, 69, 109, 591, 159],\n",
       " [12, 724, 512, 27, 161, 12, 69, 109, 591, 159, 725],\n",
       " [389, 175],\n",
       " [389, 175, 592],\n",
       " [389, 175, 592, 940],\n",
       " [389, 175, 592, 940, 390],\n",
       " [389, 175, 592, 940, 390, 12],\n",
       " [389, 175, 592, 940, 390, 12, 941],\n",
       " [389, 175, 592, 940, 390, 12, 941, 6],\n",
       " [389, 175, 592, 940, 390, 12, 941, 6, 1329],\n",
       " [389, 175, 592, 940, 390, 12, 941, 6, 1329, 726],\n",
       " [159, 391],\n",
       " [159, 391, 31],\n",
       " [159, 391, 31, 25],\n",
       " [159, 391, 31, 25, 1],\n",
       " [159, 391, 31, 25, 1, 162],\n",
       " [159, 391, 31, 25, 1, 162, 93],\n",
       " [159, 391, 31, 25, 1, 162, 93, 7],\n",
       " [159, 391, 31, 25, 1, 162, 93, 7, 12],\n",
       " [159, 391, 31, 25, 1, 162, 93, 7, 12, 346],\n",
       " [159, 391, 31, 25, 1, 162, 93, 7, 12, 346, 942],\n",
       " [159, 391, 31, 25, 1, 162, 93, 7, 12, 346, 942, 3],\n",
       " [593, 8],\n",
       " [593, 8, 34],\n",
       " [593, 8, 34, 594],\n",
       " [593, 8, 34, 594, 12],\n",
       " [593, 8, 34, 594, 12, 448],\n",
       " [593, 8, 34, 594, 12, 448, 11],\n",
       " [593, 8, 34, 594, 12, 448, 11, 4],\n",
       " [593, 8, 34, 594, 12, 448, 11, 4, 9],\n",
       " [593, 8, 34, 594, 12, 448, 11, 4, 9, 727],\n",
       " [593, 8, 34, 594, 12, 448, 11, 4, 9, 727, 513],\n",
       " [593, 8, 34, 594, 12, 448, 11, 4, 9, 727, 513, 31],\n",
       " [593, 8, 34, 594, 12, 448, 11, 4, 9, 727, 513, 31, 6],\n",
       " [593, 8, 34, 594, 12, 448, 11, 4, 9, 727, 513, 31, 6, 105],\n",
       " [1330, 1331],\n",
       " [1330, 1331, 3],\n",
       " [1330, 1331, 3, 1332],\n",
       " [1330, 1331, 3, 1332, 10],\n",
       " [1330, 1331, 3, 1332, 10, 12],\n",
       " [1330, 1331, 3, 1332, 10, 12, 392],\n",
       " [1330, 1331, 3, 1332, 10, 12, 392, 1333],\n",
       " [1330, 1331, 3, 1332, 10, 12, 392, 1333, 943],\n",
       " [1330, 1331, 3, 1332, 10, 12, 392, 1333, 943, 8],\n",
       " [1330, 1331, 3, 1332, 10, 12, 392, 1333, 943, 8, 728],\n",
       " [12, 595],\n",
       " [12, 595, 944],\n",
       " [12, 595, 944, 31],\n",
       " [12, 595, 944, 31, 15],\n",
       " [12, 595, 944, 31, 15, 12],\n",
       " [12, 595, 944, 31, 15, 12, 596],\n",
       " [12, 595, 944, 31, 15, 12, 596, 597],\n",
       " [12, 595, 944, 31, 15, 12, 596, 597, 3],\n",
       " [12, 595, 944, 31, 15, 12, 596, 597, 3, 596],\n",
       " [12, 595, 944, 31, 15, 12, 596, 597, 3, 596, 1334],\n",
       " [12, 595, 944, 31, 15, 12, 596, 597, 3, 596, 1334, 97],\n",
       " [12, 595, 944, 31, 15, 12, 596, 597, 3, 596, 1334, 97, 226],\n",
       " [12, 595, 944, 31, 15, 12, 596, 597, 3, 596, 1334, 97, 226, 313],\n",
       " [598, 12],\n",
       " [598, 12, 599],\n",
       " [598, 12, 599, 596],\n",
       " [598, 12, 599, 596, 1335],\n",
       " [598, 12, 599, 596, 1335, 11],\n",
       " [598, 12, 599, 596, 1335, 11, 314],\n",
       " [598, 12, 599, 596, 1335, 11, 314, 1],\n",
       " [598, 12, 599, 596, 1335, 11, 314, 1, 176],\n",
       " [598, 12, 599, 596, 1335, 11, 314, 1, 176, 6],\n",
       " [598, 12, 599, 596, 1335, 11, 314, 1, 176, 6, 18],\n",
       " [598, 12, 599, 596, 1335, 11, 314, 1, 176, 6, 18, 393],\n",
       " [598, 12, 599, 596, 1335, 11, 314, 1, 176, 6, 18, 393, 600],\n",
       " [250, 1],\n",
       " [250, 1, 1336],\n",
       " [14, 44],\n",
       " [14, 44, 163],\n",
       " [14, 44, 163, 2],\n",
       " [14, 44, 163, 2, 140],\n",
       " [14, 44, 163, 2, 140, 31],\n",
       " [14, 44, 163, 2, 140, 31, 1],\n",
       " [14, 44, 163, 2, 140, 31, 1, 164],\n",
       " [14, 44, 163, 2, 140, 31, 1, 164, 22],\n",
       " [14, 44, 163, 2, 140, 31, 1, 164, 22, 1],\n",
       " [14, 44, 163, 2, 140, 31, 1, 164, 22, 1, 945],\n",
       " [14, 44, 163, 2, 140, 31, 1, 164, 22, 1, 945, 946],\n",
       " [947, 6],\n",
       " [947, 6, 729],\n",
       " [947, 6, 729, 29],\n",
       " [947, 6, 729, 29, 30],\n",
       " [947, 6, 729, 29, 30, 165],\n",
       " [947, 6, 729, 29, 30, 165, 948],\n",
       " [947, 6, 729, 29, 30, 165, 948, 1],\n",
       " [947, 6, 729, 29, 30, 165, 948, 1, 949],\n",
       " [947, 6, 729, 29, 30, 165, 948, 1, 949, 94],\n",
       " [947, 6, 729, 29, 30, 165, 948, 1, 949, 94, 77],\n",
       " [947, 6, 729, 29, 30, 165, 948, 1, 949, 94, 77, 17],\n",
       " [947, 6, 729, 29, 30, 165, 948, 1, 949, 94, 77, 17, 166],\n",
       " [110, 730],\n",
       " [110, 730, 114],\n",
       " [110, 730, 114, 58],\n",
       " [110, 730, 114, 58, 33],\n",
       " [110, 730, 114, 58, 33, 63],\n",
       " [110, 730, 114, 58, 33, 63, 227],\n",
       " [110, 730, 114, 58, 33, 63, 227, 12],\n",
       " [110, 730, 114, 58, 33, 63, 227, 12, 69],\n",
       " [110, 730, 114, 58, 33, 63, 227, 12, 69, 601],\n",
       " [110, 730, 114, 58, 33, 63, 227, 12, 69, 601, 177],\n",
       " [110, 730, 114, 58, 33, 63, 227, 12, 69, 601, 177, 3],\n",
       " [110, 730, 114, 58, 33, 63, 227, 12, 69, 601, 177, 3, 449],\n",
       " [110, 730, 114, 58, 33, 63, 227, 12, 69, 601, 177, 3, 449, 26],\n",
       " [40, 1337],\n",
       " [40, 1337, 4],\n",
       " [40, 1337, 4, 98],\n",
       " [40, 1337, 4, 98, 21],\n",
       " [40, 1337, 4, 98, 21, 11],\n",
       " [40, 1337, 4, 98, 21, 11, 7],\n",
       " [40, 1337, 4, 98, 21, 11, 7, 118],\n",
       " [40, 1337, 4, 98, 21, 11, 7, 118, 4],\n",
       " [40, 1337, 4, 98, 21, 11, 7, 118, 4, 7],\n",
       " [40, 1337, 4, 98, 21, 11, 7, 118, 4, 7, 450],\n",
       " [40, 1337, 4, 98, 21, 11, 7, 118, 4, 7, 450, 2],\n",
       " [71, 134],\n",
       " [71, 134, 4],\n",
       " [71, 134, 4, 7],\n",
       " [71, 134, 4, 7, 151],\n",
       " [71, 134, 4, 7, 151, 2],\n",
       " [71, 134, 4, 7, 151, 2, 451],\n",
       " [71, 134, 4, 7, 151, 2, 451, 25],\n",
       " [71, 134, 4, 7, 151, 2, 451, 25, 5],\n",
       " [71, 134, 4, 7, 151, 2, 451, 25, 5, 167],\n",
       " [71, 134, 4, 7, 151, 2, 451, 25, 5, 167, 3],\n",
       " [71, 134, 4, 7, 151, 2, 451, 25, 5, 167, 3, 10],\n",
       " [71, 134, 4, 7, 151, 2, 451, 25, 5, 167, 3, 10, 5],\n",
       " [71, 134, 4, 7, 151, 2, 451, 25, 5, 167, 3, 10, 5, 315],\n",
       " [296, 347],\n",
       " [296, 347, 72],\n",
       " [296, 347, 72, 49],\n",
       " [296, 347, 72, 49, 11],\n",
       " [296, 347, 72, 49, 11, 348],\n",
       " [296, 347, 72, 49, 11, 348, 297],\n",
       " [296, 347, 72, 49, 11, 348, 297, 251],\n",
       " [296, 347, 72, 49, 11, 348, 297, 251, 4],\n",
       " [296, 347, 72, 49, 11, 348, 297, 251, 4, 514],\n",
       " [42, 147],\n",
       " [42, 147, 5],\n",
       " [42, 147, 5, 167],\n",
       " [42, 147, 5, 167, 4],\n",
       " [42, 147, 5, 167, 4, 178],\n",
       " [42, 147, 5, 167, 4, 178, 731],\n",
       " [42, 147, 5, 167, 4, 178, 731, 45],\n",
       " [42, 147, 5, 167, 4, 178, 731, 45, 2],\n",
       " [42, 147, 5, 167, 4, 178, 731, 45, 2, 89],\n",
       " [42, 147, 5, 167, 4, 178, 731, 45, 2, 89, 4],\n",
       " [42, 147, 5, 167, 4, 178, 731, 45, 2, 89, 4, 7],\n",
       " [42, 147, 5, 167, 4, 178, 731, 45, 2, 89, 4, 7, 4],\n",
       " [179, 32],\n",
       " [179, 32, 252],\n",
       " [179, 32, 252, 8],\n",
       " [179, 32, 252, 8, 12],\n",
       " [179, 32, 252, 8, 12, 1338],\n",
       " [179, 32, 252, 8, 12, 1338, 213],\n",
       " [179, 32, 252, 8, 12, 1338, 213, 316],\n",
       " [179, 32, 252, 8, 12, 1338, 213, 316, 5],\n",
       " [179, 32, 252, 8, 12, 1338, 213, 316, 5, 168],\n",
       " [179, 32, 252, 8, 12, 1338, 213, 316, 5, 168, 37],\n",
       " [179, 32, 252, 8, 12, 1338, 213, 316, 5, 168, 37, 11],\n",
       " [179, 32, 252, 8, 12, 1338, 213, 316, 5, 168, 37, 11, 4],\n",
       " [515, 32],\n",
       " [515, 32, 2],\n",
       " [515, 32, 2, 140],\n",
       " [515, 32, 2, 140, 22],\n",
       " [515, 32, 2, 140, 22, 1],\n",
       " [515, 32, 2, 140, 22, 1, 1339],\n",
       " [515, 32, 2, 140, 22, 1, 1339, 150],\n",
       " [515, 32, 2, 140, 22, 1, 1339, 150, 3],\n",
       " [515, 32, 2, 140, 22, 1, 1339, 150, 3, 61],\n",
       " [515, 32, 2, 140, 22, 1, 1339, 150, 3, 61, 317],\n",
       " [515, 32, 2, 140, 22, 1, 1339, 150, 3, 61, 317, 55],\n",
       " [4, 180],\n",
       " [4, 180, 2],\n",
       " [4, 180, 2, 166],\n",
       " [4, 180, 2, 166, 12],\n",
       " [4, 180, 2, 166, 12, 1340],\n",
       " [4, 180, 2, 166, 12, 1340, 945],\n",
       " [4, 180, 2, 166, 12, 1340, 945, 271],\n",
       " [4, 180, 2, 166, 12, 1340, 945, 271, 34],\n",
       " [4, 180, 2, 166, 12, 1340, 945, 271, 34, 11],\n",
       " [4, 180, 2, 166, 12, 1340, 945, 271, 34, 11, 4],\n",
       " [4, 180, 2, 166, 12, 1340, 945, 271, 34, 11, 4, 9],\n",
       " [4, 180, 2, 166, 12, 1340, 945, 271, 34, 11, 4, 9, 152],\n",
       " [4, 180, 2, 166, 12, 1340, 945, 271, 34, 11, 4, 9, 152, 214],\n",
       " [452, 453],\n",
       " [452, 453, 4],\n",
       " [452, 453, 4, 98],\n",
       " [452, 453, 4, 98, 57],\n",
       " [452, 453, 4, 98, 57, 12],\n",
       " [452, 453, 4, 98, 57, 12, 1341],\n",
       " [452, 453, 4, 98, 57, 12, 1341, 1342],\n",
       " [452, 453, 4, 98, 57, 12, 1341, 1342, 8],\n",
       " [452, 453, 4, 98, 57, 12, 1341, 1342, 8, 123],\n",
       " [452, 453, 4, 98, 57, 12, 1341, 1342, 8, 123, 11],\n",
       " [452, 453, 4, 98, 57, 12, 1341, 1342, 8, 123, 11, 602],\n",
       " [950, 346],\n",
       " [950, 346, 115],\n",
       " [950, 346, 115, 10],\n",
       " [950, 346, 115, 10, 3],\n",
       " [950, 346, 115, 10, 3, 115],\n",
       " [950, 346, 115, 10, 3, 115, 31],\n",
       " [950, 346, 115, 10, 3, 115, 31, 228],\n",
       " [950, 346, 115, 10, 3, 115, 31, 228, 215],\n",
       " [950, 346, 115, 10, 3, 115, 31, 228, 215, 84],\n",
       " [950, 346, 115, 10, 3, 115, 31, 228, 215, 84, 40],\n",
       " [1343, 87],\n",
       " [1343, 87, 52],\n",
       " [1343, 87, 52, 298],\n",
       " [1343, 87, 52, 298, 65],\n",
       " [1343, 87, 52, 298, 65, 228],\n",
       " [1343, 87, 52, 298, 65, 228, 202],\n",
       " [1343, 87, 52, 298, 65, 228, 202, 299],\n",
       " [1343, 87, 52, 298, 65, 228, 202, 299, 215],\n",
       " [1343, 87, 52, 298, 65, 228, 202, 299, 215, 22],\n",
       " [1343, 87, 52, 298, 65, 228, 202, 299, 215, 22, 169],\n",
       " [1343, 87, 52, 298, 65, 228, 202, 299, 215, 22, 169, 3],\n",
       " [1343, 87, 52, 298, 65, 228, 202, 299, 215, 22, 169, 3, 25],\n",
       " [732, 6],\n",
       " [732, 6, 11],\n",
       " [732, 6, 11, 603],\n",
       " [732, 6, 11, 603, 1],\n",
       " [732, 6, 11, 603, 1, 951],\n",
       " [732, 6, 11, 603, 1, 951, 6],\n",
       " [732, 6, 11, 603, 1, 951, 6, 346],\n",
       " [732, 6, 11, 603, 1, 951, 6, 346, 952],\n",
       " [732, 6, 11, 603, 1, 951, 6, 346, 952, 58],\n",
       " [732, 6, 11, 603, 1, 951, 6, 346, 952, 58, 272],\n",
       " [318, 1344],\n",
       " [318, 1344, 733],\n",
       " [318, 1344, 733, 3],\n",
       " [318, 1344, 733, 3, 1345],\n",
       " [318, 1344, 733, 3, 1345, 229],\n",
       " [318, 1344, 733, 3, 1345, 229, 1346],\n",
       " [318, 1344, 733, 3, 1345, 229, 1346, 15],\n",
       " [318, 1344, 733, 3, 1345, 229, 1346, 15, 394],\n",
       " [454, 26],\n",
       " [454, 26, 1],\n",
       " [454, 26, 1, 62],\n",
       " [454, 26, 1, 62, 37],\n",
       " [454, 26, 1, 62, 37, 11],\n",
       " [454, 26, 1, 62, 37, 11, 66],\n",
       " [454, 26, 1, 62, 37, 11, 66, 216],\n",
       " [454, 26, 1, 62, 37, 11, 66, 216, 152],\n",
       " [454, 26, 1, 62, 37, 11, 66, 216, 152, 72],\n",
       " [454, 26, 1, 62, 37, 11, 66, 216, 152, 72, 2],\n",
       " [454, 26, 1, 62, 37, 11, 66, 216, 152, 72, 2, 194],\n",
       " [454, 26, 1, 62, 37, 11, 66, 216, 152, 72, 2, 194, 349],\n",
       " [454, 26, 1, 62, 37, 11, 66, 216, 152, 72, 2, 194, 349, 76],\n",
       " [170, 516],\n",
       " [170, 516, 15],\n",
       " [170, 516, 15, 59],\n",
       " [170, 516, 15, 59, 8],\n",
       " [170, 516, 15, 59, 8, 216],\n",
       " [170, 516, 15, 59, 8, 216, 26],\n",
       " [170, 516, 15, 59, 8, 216, 26, 78],\n",
       " [170, 516, 15, 59, 8, 216, 26, 78, 2],\n",
       " [170, 516, 15, 59, 8, 216, 26, 78, 2, 1347],\n",
       " [170, 516, 15, 59, 8, 216, 26, 78, 2, 1347, 4],\n",
       " [170, 516, 15, 59, 8, 216, 26, 78, 2, 1347, 4, 214],\n",
       " [170, 516, 15, 59, 8, 216, 26, 78, 2, 1347, 4, 214, 12],\n",
       " [604, 953],\n",
       " [604, 953, 47],\n",
       " [604, 953, 47, 25],\n",
       " [604, 953, 47, 25, 5],\n",
       " [604, 953, 47, 25, 5, 722],\n",
       " [604, 953, 47, 25, 5, 722, 253],\n",
       " [604, 953, 47, 25, 5, 722, 253, 42],\n",
       " [604, 953, 47, 25, 5, 722, 253, 42, 141],\n",
       " [604, 953, 47, 25, 5, 722, 253, 42, 141, 47],\n",
       " [604, 953, 47, 25, 5, 722, 253, 42, 141, 47, 25],\n",
       " [604, 953, 47, 25, 5, 722, 253, 42, 141, 47, 25, 5],\n",
       " [604, 953, 47, 25, 5, 722, 253, 42, 141, 47, 25, 5, 45],\n",
       " [250, 1],\n",
       " [250, 1, 1348],\n",
       " [250, 1, 1348, 37],\n",
       " [250, 1, 1348, 37, 11],\n",
       " [250, 1, 1348, 37, 11, 4],\n",
       " [250, 1, 1348, 37, 11, 4, 29],\n",
       " [250, 1, 1348, 37, 11, 4, 29, 734],\n",
       " [250, 1, 1348, 37, 11, 4, 29, 734, 5],\n",
       " [250, 1, 1348, 37, 11, 4, 29, 734, 5, 85],\n",
       " [250, 1, 1348, 37, 11, 4, 29, 734, 5, 85, 153],\n",
       " [250, 1, 1348, 37, 11, 4, 29, 734, 5, 85, 153, 241],\n",
       " [89, 1],\n",
       " [89, 1, 953],\n",
       " [89, 1, 953, 7],\n",
       " [89, 1, 953, 7, 3],\n",
       " [89, 1, 953, 7, 3, 254],\n",
       " [89, 1, 953, 7, 3, 254, 11],\n",
       " [89, 1, 953, 7, 3, 254, 11, 8],\n",
       " [89, 1, 953, 7, 3, 254, 11, 8, 7],\n",
       " [89, 1, 953, 7, 3, 254, 11, 8, 7, 314],\n",
       " [89, 1, 953, 7, 3, 254, 11, 8, 7, 314, 15],\n",
       " [89, 1, 953, 7, 3, 254, 11, 8, 7, 314, 15, 1349],\n",
       " [89, 1, 953, 7, 3, 254, 11, 8, 7, 314, 15, 1349, 6],\n",
       " [89, 1, 953, 7, 3, 254, 11, 8, 7, 314, 15, 1349, 6, 69],\n",
       " [395, 1350],\n",
       " [395, 1350, 94],\n",
       " [395, 1350, 94, 4],\n",
       " [395, 1350, 94, 4, 273],\n",
       " [395, 1350, 94, 4, 273, 194],\n",
       " [395, 1350, 94, 4, 273, 194, 57],\n",
       " [395, 1350, 94, 4, 273, 194, 57, 2],\n",
       " [395, 1350, 94, 4, 273, 194, 57, 2, 135],\n",
       " [395, 1350, 94, 4, 273, 194, 57, 2, 135, 6],\n",
       " [395, 1350, 94, 4, 273, 194, 57, 2, 135, 6, 3],\n",
       " [395, 1350, 94, 4, 273, 194, 57, 2, 135, 6, 3, 55],\n",
       " [395, 1350, 94, 4, 273, 194, 57, 2, 135, 6, 3, 55, 4],\n",
       " [395, 1350, 94, 4, 273, 194, 57, 2, 135, 6, 3, 55, 4, 252],\n",
       " [2, 166],\n",
       " [2, 166, 1],\n",
       " [2, 166, 1, 350],\n",
       " [2, 166, 1, 350, 15],\n",
       " [2, 166, 1, 350, 15, 53],\n",
       " [2, 166, 1, 350, 15, 53, 6],\n",
       " [2, 166, 1, 350, 15, 53, 6, 5],\n",
       " [2, 166, 1, 350, 15, 53, 6, 5, 150],\n",
       " [2, 166, 1, 350, 15, 53, 6, 5, 150, 4],\n",
       " [2, 166, 1, 350, 15, 53, 6, 5, 150, 4, 605],\n",
       " [2, 166, 1, 350, 15, 53, 6, 5, 150, 4, 605, 8],\n",
       " [2, 166, 1, 350, 15, 53, 6, 5, 150, 4, 605, 8, 255],\n",
       " [2, 166, 1, 350, 15, 53, 6, 5, 150, 4, 605, 8, 255, 45],\n",
       " [134, 13],\n",
       " [134, 13, 106],\n",
       " [134, 13, 106, 13],\n",
       " [134, 13, 106, 13, 4],\n",
       " [134, 13, 106, 13, 4, 606],\n",
       " [134, 13, 106, 13, 4, 606, 8],\n",
       " [134, 13, 106, 13, 4, 606, 8, 4],\n",
       " [134, 13, 106, 13, 4, 606, 8, 4, 7],\n",
       " [134, 13, 106, 13, 4, 606, 8, 4, 7, 517],\n",
       " [134, 13, 106, 13, 4, 606, 8, 4, 7, 517, 64],\n",
       " [134, 13, 106, 13, 4, 606, 8, 4, 7, 517, 64, 12],\n",
       " [134, 13, 106, 13, 4, 606, 8, 4, 7, 517, 64, 12, 735],\n",
       " [134, 13, 106, 13, 4, 606, 8, 4, 7, 517, 64, 12, 735, 1351],\n",
       " [4, 736],\n",
       " [4, 736, 45],\n",
       " [4, 736, 45, 49],\n",
       " [4, 736, 45, 49, 5],\n",
       " [4, 736, 45, 49, 5, 1352],\n",
       " [4, 736, 45, 49, 5, 1352, 348],\n",
       " [4, 736, 45, 49, 5, 1352, 348, 396],\n",
       " [4, 736, 45, 49, 5, 1352, 348, 396, 47],\n",
       " [4, 736, 45, 49, 5, 1352, 348, 396, 47, 455],\n",
       " [4, 736, 45, 49, 5, 1352, 348, 396, 47, 455, 26],\n",
       " [4, 736, 45, 49, 5, 1352, 348, 396, 47, 455, 26, 1],\n",
       " [62, 4],\n",
       " [62, 4, 98],\n",
       " [62, 4, 98, 8],\n",
       " [62, 4, 98, 8, 954],\n",
       " [62, 4, 98, 8, 954, 66],\n",
       " [62, 4, 98, 8, 954, 66, 955],\n",
       " [62, 4, 98, 8, 954, 66, 955, 956],\n",
       " [62, 4, 98, 8, 954, 66, 955, 956, 148],\n",
       " [62, 4, 98, 8, 954, 66, 955, 956, 148, 2],\n",
       " [62, 4, 98, 8, 954, 66, 955, 956, 148, 2, 72],\n",
       " [62, 4, 98, 8, 954, 66, 955, 956, 148, 2, 72, 154],\n",
       " [227, 73],\n",
       " [227, 73, 346],\n",
       " [227, 73, 346, 737],\n",
       " [227, 73, 346, 737, 607],\n",
       " [227, 73, 346, 737, 607, 12],\n",
       " [227, 73, 346, 737, 607, 12, 242],\n",
       " [227, 73, 346, 737, 607, 12, 242, 6],\n",
       " [227, 73, 346, 737, 607, 12, 242, 6, 1353],\n",
       " [227, 73, 346, 737, 607, 12, 242, 6, 1353, 23],\n",
       " [957, 456],\n",
       " [957, 456, 63],\n",
       " [957, 456, 63, 78],\n",
       " [957, 456, 63, 78, 45],\n",
       " [957, 456, 63, 78, 45, 2],\n",
       " [957, 456, 63, 78, 45, 2, 1],\n",
       " [957, 456, 63, 78, 45, 2, 1, 1354],\n",
       " [957, 456, 63, 78, 45, 2, 1, 1354, 958],\n",
       " [957, 456, 63, 78, 45, 2, 1, 1354, 958, 397],\n",
       " [957, 456, 63, 78, 45, 2, 1, 1354, 958, 397, 1],\n",
       " [957, 456, 63, 78, 45, 2, 1, 1354, 958, 397, 1, 139],\n",
       " [2, 1355],\n",
       " [2, 1355, 31],\n",
       " [2, 1355, 31, 1],\n",
       " [2, 1355, 31, 1, 1356],\n",
       " [2, 1355, 31, 1, 1356, 230],\n",
       " [2, 1355, 31, 1, 1356, 230, 130],\n",
       " [2, 1355, 31, 1, 1356, 230, 130, 457],\n",
       " [2, 1355, 31, 1, 1356, 230, 130, 457, 178],\n",
       " [2, 1355, 31, 1, 1356, 230, 130, 457, 178, 74],\n",
       " [2, 1355, 31, 1, 1356, 230, 130, 457, 178, 74, 738],\n",
       " [34, 351],\n",
       " [34, 351, 54],\n",
       " [34, 351, 54, 959],\n",
       " [34, 351, 54, 959, 63],\n",
       " [34, 351, 54, 959, 63, 608],\n",
       " [34, 351, 54, 959, 63, 608, 2],\n",
       " [34, 351, 54, 959, 63, 608, 2, 67],\n",
       " [34, 351, 54, 959, 63, 608, 2, 67, 458],\n",
       " [34, 351, 54, 959, 63, 608, 2, 67, 458, 11],\n",
       " [34, 351, 54, 959, 63, 608, 2, 67, 458, 11, 15],\n",
       " [34, 351, 54, 959, 63, 608, 2, 67, 458, 11, 15, 149],\n",
       " [319, 518],\n",
       " [319, 518, 72],\n",
       " [319, 518, 72, 1357],\n",
       " [319, 518, 72, 1357, 31],\n",
       " [319, 518, 72, 1357, 31, 25],\n",
       " [319, 518, 72, 1357, 31, 25, 1],\n",
       " [319, 518, 72, 1357, 31, 25, 1, 739],\n",
       " [319, 518, 72, 1357, 31, 25, 1, 739, 21],\n",
       " [319, 518, 72, 1357, 31, 25, 1, 739, 21, 97],\n",
       " [319, 518, 72, 1357, 31, 25, 1, 739, 21, 97, 1358],\n",
       " [319, 518, 72, 1357, 31, 25, 1, 739, 21, 97, 1358, 398],\n",
       " [319, 518, 72, 1357, 31, 25, 1, 739, 21, 97, 1358, 398, 11],\n",
       " [19, 30],\n",
       " [19, 30, 1],\n",
       " [19, 30, 1, 609],\n",
       " [19, 30, 1, 609, 274],\n",
       " [19, 30, 1, 609, 274, 23],\n",
       " [19, 30, 1, 609, 274, 23, 193],\n",
       " [19, 30, 1, 609, 274, 23, 193, 33],\n",
       " [19, 30, 1, 609, 274, 23, 193, 33, 63],\n",
       " [19, 30, 1, 609, 274, 23, 193, 33, 63, 273],\n",
       " [19, 30, 1, 609, 274, 23, 193, 33, 63, 273, 32],\n",
       " [19, 30, 1, 609, 274, 23, 193, 33, 63, 273, 32, 149],\n",
       " [19, 30, 1, 609, 274, 23, 193, 33, 63, 273, 32, 149, 127],\n",
       " [19, 30, 1, 609, 274, 23, 193, 33, 63, 273, 32, 149, 127, 2],\n",
       " [155, 58],\n",
       " [155, 58, 518],\n",
       " [155, 58, 518, 32],\n",
       " [155, 58, 518, 32, 459],\n",
       " [155, 58, 518, 32, 459, 10],\n",
       " [155, 58, 518, 32, 459, 10, 149],\n",
       " [155, 58, 518, 32, 459, 10, 149, 352],\n",
       " [155, 58, 518, 32, 459, 10, 149, 352, 12],\n",
       " [155, 58, 518, 32, 459, 10, 149, 352, 12, 142],\n",
       " [155, 58, 518, 32, 459, 10, 149, 352, 12, 142, 62],\n",
       " [155, 58, 518, 32, 459, 10, 149, 352, 12, 142, 62, 740],\n",
       " [155, 58, 518, 32, 459, 10, 149, 352, 12, 142, 62, 740, 518],\n",
       " [155, 58, 518, 32, 459, 10, 149, 352, 12, 142, 62, 740, 518, 32],\n",
       " [519, 47],\n",
       " [519, 47, 2],\n",
       " [519, 47, 2, 1],\n",
       " [519, 47, 2, 1, 319],\n",
       " [519, 47, 2, 1, 319, 3],\n",
       " [519, 47, 2, 1, 319, 3, 320],\n",
       " [519, 47, 2, 1, 319, 3, 320, 17],\n",
       " [519, 47, 2, 1, 319, 3, 320, 17, 67],\n",
       " [519, 47, 2, 1, 319, 3, 320, 17, 67, 57],\n",
       " [519, 47, 2, 1, 319, 3, 320, 17, 67, 57, 63],\n",
       " [519, 47, 2, 1, 319, 3, 320, 17, 67, 57, 63, 155],\n",
       " [519, 47, 2, 1, 319, 3, 320, 17, 67, 57, 63, 155, 460],\n",
       " [519, 47, 2, 1, 319, 3, 320, 17, 67, 57, 63, 155, 460, 17],\n",
       " [116, 63],\n",
       " [116, 63, 19],\n",
       " [116, 63, 19, 119],\n",
       " [116, 63, 19, 119, 17],\n",
       " [116, 63, 19, 119, 17, 194],\n",
       " [116, 63, 19, 119, 17, 194, 67],\n",
       " [116, 63, 19, 119, 17, 194, 67, 57],\n",
       " [116, 63, 19, 119, 17, 194, 67, 57, 63],\n",
       " [116, 63, 19, 119, 17, 194, 67, 57, 63, 166],\n",
       " [116, 63, 19, 119, 17, 194, 67, 57, 63, 166, 960],\n",
       " [116, 63, 19, 119, 17, 194, 67, 57, 63, 166, 960, 353],\n",
       " [116, 63, 19, 119, 17, 194, 67, 57, 63, 166, 960, 353, 167],\n",
       " [160, 5],\n",
       " [160, 5, 321],\n",
       " [160, 5, 321, 3],\n",
       " [160, 5, 321, 3, 203],\n",
       " [160, 5, 321, 3, 203, 12],\n",
       " [160, 5, 321, 3, 203, 12, 1359],\n",
       " [160, 5, 321, 3, 203, 12, 1359, 461],\n",
       " [160, 5, 321, 3, 203, 12, 1359, 461, 6],\n",
       " [160, 5, 321, 3, 203, 12, 1359, 461, 6, 215],\n",
       " [160, 5, 321, 3, 203, 12, 1359, 461, 6, 215, 2],\n",
       " [160, 5, 321, 3, 203, 12, 1359, 461, 6, 215, 2, 30],\n",
       " [160, 5, 321, 3, 203, 12, 1359, 461, 6, 215, 2, 30, 738],\n",
       " [160, 5, 321, 3, 203, 12, 1359, 461, 6, 215, 2, 30, 738, 47],\n",
       " [34, 22],\n",
       " [34, 22, 202],\n",
       " [34, 22, 202, 321],\n",
       " [34, 22, 202, 321, 610],\n",
       " [34, 22, 202, 321, 610, 91],\n",
       " [34, 22, 202, 321, 610, 91, 22],\n",
       " [34, 22, 202, 321, 610, 91, 22, 202],\n",
       " [34, 22, 202, 321, 610, 91, 22, 202, 1360],\n",
       " [34, 22, 202, 321, 610, 91, 22, 202, 1360, 28],\n",
       " [34, 22, 202, 321, 610, 91, 22, 202, 1360, 28, 47],\n",
       " [34, 22, 202, 321, 610, 91, 22, 202, 1360, 28, 47, 34],\n",
       " [256, 55],\n",
       " [256, 55, 66],\n",
       " [256, 55, 66, 32],\n",
       " [256, 55, 66, 32, 2],\n",
       " [256, 55, 66, 32, 2, 78],\n",
       " [256, 55, 66, 32, 2, 78, 167],\n",
       " [256, 55, 66, 32, 2, 78, 167, 47],\n",
       " [256, 55, 66, 32, 2, 78, 167, 47, 243],\n",
       " [256, 55, 66, 32, 2, 78, 167, 47, 243, 134],\n",
       " [256, 55, 66, 32, 2, 78, 167, 47, 243, 134, 1],\n",
       " [256, 55, 66, 32, 2, 78, 167, 47, 243, 134, 1, 319],\n",
       " [256, 55, 66, 32, 2, 78, 167, 47, 243, 134, 1, 319, 123],\n",
       " [251, 6],\n",
       " [251, 6, 741],\n",
       " [251, 6, 741, 99],\n",
       " [251, 6, 741, 99, 603],\n",
       " [251, 6, 741, 99, 603, 74],\n",
       " [251, 6, 741, 99, 603, 74, 79],\n",
       " [251, 6, 741, 99, 603, 74, 79, 611],\n",
       " [251, 6, 741, 99, 603, 74, 79, 611, 181],\n",
       " [251, 6, 741, 99, 603, 74, 79, 611, 181, 602],\n",
       " [251, 6, 741, 99, 603, 74, 79, 611, 181, 602, 148],\n",
       " [251, 6, 741, 99, 603, 74, 79, 611, 181, 602, 148, 1],\n",
       " [244, 322],\n",
       " [244, 322, 2],\n",
       " [244, 322, 2, 520],\n",
       " [244, 322, 2, 520, 160],\n",
       " [244, 322, 2, 520, 160, 149],\n",
       " [244, 322, 2, 520, 160, 149, 1361],\n",
       " [244, 322, 2, 520, 160, 149, 1361, 742],\n",
       " [244, 322, 2, 520, 160, 149, 1361, 742, 2],\n",
       " [244, 322, 2, 520, 160, 149, 1361, 742, 2, 17],\n",
       " [244, 322, 2, 520, 160, 149, 1361, 742, 2, 17, 323],\n",
       " [244, 322, 2, 520, 160, 149, 1361, 742, 2, 17, 323, 462],\n",
       " [244, 322, 2, 520, 160, 149, 1361, 742, 2, 17, 323, 462, 76],\n",
       " [743, 399],\n",
       " [743, 399, 63],\n",
       " [743, 399, 63, 1362],\n",
       " [743, 399, 63, 1362, 275],\n",
       " [743, 399, 63, 1362, 275, 961],\n",
       " [743, 399, 63, 1362, 275, 961, 57],\n",
       " [743, 399, 63, 1362, 275, 961, 57, 354],\n",
       " [743, 399, 63, 1362, 275, 961, 57, 354, 71],\n",
       " [743, 399, 63, 1362, 275, 961, 57, 354, 71, 275],\n",
       " [743, 399, 63, 1362, 275, 961, 57, 354, 71, 275, 55],\n",
       " [354, 135],\n",
       " [354, 135, 1],\n",
       " [354, 135, 1, 744],\n",
       " [354, 135, 1, 744, 612],\n",
       " [354, 135, 1, 744, 612, 75],\n",
       " [354, 135, 1, 744, 612, 75, 6],\n",
       " [354, 135, 1, 744, 612, 75, 6, 26],\n",
       " [354, 135, 1, 744, 612, 75, 6, 26, 111],\n",
       " [354, 135, 1, 744, 612, 75, 6, 26, 111, 602],\n",
       " [354, 135, 1, 744, 612, 75, 6, 26, 111, 602, 148],\n",
       " [354, 135, 1, 744, 612, 75, 6, 26, 111, 602, 148, 2],\n",
       " [354, 135, 1, 744, 612, 75, 6, 26, 111, 602, 148, 2, 72],\n",
       " [354, 135, 1, 744, 612, 75, 6, 26, 111, 602, 148, 2, 72, 47],\n",
       " [149, 318],\n",
       " [149, 318, 1363],\n",
       " [149, 318, 1363, 22],\n",
       " [149, 318, 1363, 22, 462],\n",
       " [3, 4],\n",
       " [3, 4, 113],\n",
       " [3, 4, 113, 100],\n",
       " [3, 4, 113, 100, 22],\n",
       " [3, 4, 113, 100, 22, 1],\n",
       " [3, 4, 113, 100, 22, 1, 463],\n",
       " [3, 4, 113, 100, 22, 1, 463, 521],\n",
       " [3, 4, 113, 100, 22, 1, 463, 521, 1364],\n",
       " [3, 4, 113, 100, 22, 1, 463, 521, 1364, 25],\n",
       " [3, 4, 113, 100, 22, 1, 463, 521, 1364, 25, 1],\n",
       " [3, 4, 113, 100, 22, 1, 463, 521, 1364, 25, 1, 231],\n",
       " [3, 4, 113, 100, 22, 1, 463, 521, 1364, 25, 1, 231, 6],\n",
       " [324, 453],\n",
       " [324, 453, 10],\n",
       " [324, 453, 10, 1365],\n",
       " [324, 453, 10, 1365, 4],\n",
       " [324, 453, 10, 1365, 4, 98],\n",
       " [324, 453, 10, 1365, 4, 98, 8],\n",
       " [324, 453, 10, 1365, 4, 98, 8, 7],\n",
       " [324, 453, 10, 1365, 4, 98, 8, 7, 325],\n",
       " [324, 453, 10, 1365, 4, 98, 8, 7, 325, 464],\n",
       " [324, 453, 10, 1365, 4, 98, 8, 7, 325, 464, 743],\n",
       " [324, 453, 10, 1365, 4, 98, 8, 7, 325, 464, 743, 3],\n",
       " [324, 453, 10, 1365, 4, 98, 8, 7, 325, 464, 743, 3, 1],\n",
       " [171, 56],\n",
       " [171, 56, 522],\n",
       " [171, 56, 522, 276],\n",
       " [171, 56, 522, 276, 613],\n",
       " [171, 56, 522, 276, 613, 8],\n",
       " [171, 56, 522, 276, 613, 8, 7],\n",
       " [171, 56, 522, 276, 613, 8, 7, 48],\n",
       " [171, 56, 522, 276, 613, 8, 7, 48, 257],\n",
       " [171, 56, 522, 276, 613, 8, 7, 48, 257, 65],\n",
       " [171, 56, 522, 276, 613, 8, 7, 48, 257, 65, 325],\n",
       " [464, 52],\n",
       " [464, 52, 84],\n",
       " [464, 52, 84, 465],\n",
       " [464, 52, 84, 465, 2],\n",
       " [464, 52, 84, 465, 2, 326],\n",
       " [464, 52, 84, 465, 2, 326, 9],\n",
       " [464, 52, 84, 465, 2, 326, 9, 1],\n",
       " [464, 52, 84, 465, 2, 326, 9, 1, 463],\n",
       " [464, 52, 84, 465, 2, 326, 9, 1, 463, 521],\n",
       " [464, 52, 84, 465, 2, 326, 9, 1, 463, 521, 20],\n",
       " [464, 52, 84, 465, 2, 326, 9, 1, 463, 521, 20, 962],\n",
       " [464, 52, 84, 465, 2, 326, 9, 1, 463, 521, 20, 962, 4],\n",
       " [29, 81],\n",
       " [29, 81, 28],\n",
       " [29, 81, 28, 1],\n",
       " [29, 81, 28, 1, 112],\n",
       " [29, 81, 28, 1, 112, 11],\n",
       " [29, 81, 28, 1, 112, 11, 8],\n",
       " [29, 81, 28, 1, 112, 11, 8, 9],\n",
       " [29, 81, 28, 1, 112, 11, 8, 9, 35],\n",
       " [29, 81, 28, 1, 112, 11, 8, 9, 35, 277],\n",
       " [29, 81, 28, 1, 112, 11, 8, 9, 35, 277, 23],\n",
       " [29, 81, 28, 1, 112, 11, 8, 9, 35, 277, 23, 592],\n",
       " [29, 81, 28, 1, 112, 11, 8, 9, 35, 277, 23, 592, 400],\n",
       " [29, 81, 28, 1, 112, 11, 8, 9, 35, 277, 23, 592, 400, 13],\n",
       " [29, 81, 28, 1, 112, 11, 8, 9, 35, 277, 23, 592, 400, 13, 8],\n",
       " [195, 32],\n",
       " [195, 32, 35],\n",
       " [195, 32, 35, 8],\n",
       " [195, 32, 35, 8, 245],\n",
       " [195, 32, 35, 8, 245, 179],\n",
       " [195, 32, 35, 8, 245, 179, 32],\n",
       " [195, 32, 35, 8, 245, 179, 32, 962],\n",
       " [195, 32, 35, 8, 245, 179, 32, 962, 355],\n",
       " [195, 32, 35, 8, 245, 179, 32, 962, 355, 21],\n",
       " [195, 32, 35, 8, 245, 179, 32, 962, 355, 21, 7],\n",
       " [195, 32, 35, 8, 245, 179, 32, 962, 355, 21, 7, 8],\n",
       " [182, 2],\n",
       " [182, 2, 522],\n",
       " [182, 2, 522, 227],\n",
       " [182, 2, 522, 227, 156],\n",
       " [182, 2, 522, 227, 156, 11],\n",
       " [182, 2, 522, 227, 156, 11, 217],\n",
       " [182, 2, 522, 227, 156, 11, 217, 1366],\n",
       " [182, 2, 522, 227, 156, 11, 217, 1366, 356],\n",
       " [745, 4],\n",
       " [745, 4, 9],\n",
       " [745, 4, 9, 20],\n",
       " [745, 4, 9, 20, 963],\n",
       " [745, 4, 9, 20, 963, 725],\n",
       " [745, 4, 9, 20, 963, 725, 21],\n",
       " [745, 4, 9, 20, 963, 725, 21, 218],\n",
       " [745, 4, 9, 20, 963, 725, 21, 218, 26],\n",
       " [745, 4, 9, 20, 963, 725, 21, 218, 26, 1],\n",
       " [745, 4, 9, 20, 963, 725, 21, 218, 26, 1, 52],\n",
       " [745, 4, 9, 20, 963, 725, 21, 218, 26, 1, 52, 746],\n",
       " [134, 6],\n",
       " [134, 6, 11],\n",
       " [134, 6, 11, 57],\n",
       " [134, 6, 11, 57, 195],\n",
       " [134, 6, 11, 57, 195, 4],\n",
       " [134, 6, 11, 57, 195, 4, 71],\n",
       " [134, 6, 11, 57, 195, 4, 71, 41],\n",
       " [134, 6, 11, 57, 195, 4, 71, 41, 1],\n",
       " [134, 6, 11, 57, 195, 4, 71, 41, 1, 183],\n",
       " [134, 6, 11, 57, 195, 4, 71, 41, 1, 183, 318],\n",
       " [134, 6, 11, 57, 195, 4, 71, 41, 1, 183, 318, 101],\n",
       " [134, 6, 11, 57, 195, 4, 71, 41, 1, 183, 318, 101, 22],\n",
       " [326, 33],\n",
       " [326, 33, 4],\n",
       " [326, 33, 4, 56],\n",
       " [326, 33, 4, 56, 2],\n",
       " [326, 33, 4, 56, 2, 614],\n",
       " [326, 33, 4, 56, 2, 614, 11],\n",
       " [326, 33, 4, 56, 2, 614, 11, 4],\n",
       " [326, 33, 4, 56, 2, 614, 11, 4, 19],\n",
       " [326, 33, 4, 56, 2, 614, 11, 4, 19, 32],\n",
       " [326, 33, 4, 56, 2, 614, 11, 4, 19, 32, 2],\n",
       " [326, 33, 4, 56, 2, 614, 11, 4, 19, 32, 2, 523],\n",
       " [326, 33, 4, 56, 2, 614, 11, 4, 19, 32, 2, 523, 84],\n",
       " [326, 33, 4, 56, 2, 614, 11, 4, 19, 32, 2, 523, 84, 964],\n",
       " [326, 33, 4, 56, 2, 614, 11, 4, 19, 32, 2, 523, 84, 964, 3],\n",
       " [1, 941],\n",
       " [1, 941, 6],\n",
       " [1, 941, 6, 726],\n",
       " [1, 941, 6, 726, 7],\n",
       " [1, 941, 6, 726, 7, 74],\n",
       " [1, 941, 6, 726, 7, 74, 20],\n",
       " [1, 941, 6, 726, 7, 74, 20, 1367],\n",
       " [1, 941, 6, 726, 7, 74, 20, 1367, 3],\n",
       " [1, 941, 6, 726, 7, 74, 20, 1367, 3, 4],\n",
       " [1, 941, 6, 726, 7, 74, 20, 1367, 3, 4, 51],\n",
       " [1, 941, 6, 726, 7, 74, 20, 1367, 3, 4, 51, 20],\n",
       " [1, 941, 6, 726, 7, 74, 20, 1367, 3, 4, 51, 20, 22],\n",
       " [26, 166],\n",
       " [26, 166, 965],\n",
       " [26, 166, 965, 466],\n",
       " [26, 166, 965, 466, 3],\n",
       " [26, 166, 965, 466, 3, 747],\n",
       " [26, 166, 965, 466, 3, 747, 3],\n",
       " [26, 166, 965, 466, 3, 747, 3, 48],\n",
       " [26, 166, 965, 466, 3, 747, 3, 48, 33],\n",
       " [26, 166, 965, 466, 3, 747, 3, 48, 33, 4],\n",
       " [26, 166, 965, 466, 3, 747, 3, 48, 33, 4, 51],\n",
       " [26, 166, 965, 466, 3, 747, 3, 48, 33, 4, 51, 614],\n",
       " [1, 318],\n",
       " [1, 318, 4],\n",
       " [1, 318, 4, 19],\n",
       " [1, 318, 4, 19, 20],\n",
       " [1, 318, 4, 19, 20, 748],\n",
       " [1, 318, 4, 19, 20, 748, 5],\n",
       " [1, 318, 4, 19, 20, 748, 5, 966],\n",
       " [1, 318, 4, 19, 20, 748, 5, 966, 749],\n",
       " [1, 318, 4, 19, 20, 748, 5, 966, 749, 13],\n",
       " [1, 318, 4, 19, 20, 748, 5, 966, 749, 13, 1],\n",
       " [1, 318, 4, 19, 20, 748, 5, 966, 749, 13, 1, 467],\n",
       " [967, 19],\n",
       " [967, 19, 32],\n",
       " [967, 19, 32, 35],\n",
       " [967, 19, 32, 35, 34],\n",
       " [967, 19, 32, 35, 34, 2],\n",
       " [967, 19, 32, 35, 34, 2, 81],\n",
       " [967, 19, 32, 35, 34, 2, 81, 1],\n",
       " [967, 19, 32, 35, 34, 2, 81, 1, 462],\n",
       " [967, 19, 32, 35, 34, 2, 81, 1, 462, 400],\n",
       " [967, 19, 32, 35, 34, 2, 81, 1, 462, 400, 318],\n",
       " [967, 19, 32, 35, 34, 2, 81, 1, 462, 400, 318, 78],\n",
       " [967, 19, 32, 35, 34, 2, 81, 1, 462, 400, 318, 78, 4],\n",
       " [19, 32],\n",
       " [19, 32, 232],\n",
       " [19, 32, 232, 10],\n",
       " [19, 32, 232, 10, 5],\n",
       " [19, 32, 232, 10, 5, 750],\n",
       " [19, 32, 232, 10, 5, 750, 58],\n",
       " [19, 32, 232, 10, 5, 750, 58, 36],\n",
       " [19, 32, 232, 10, 5, 750, 58, 36, 20],\n",
       " [19, 32, 232, 10, 5, 750, 58, 36, 20, 124],\n",
       " [19, 32, 232, 10, 5, 750, 58, 36, 20, 124, 34],\n",
       " [19, 32, 232, 10, 5, 750, 58, 36, 20, 124, 34, 12],\n",
       " [19, 32, 232, 10, 5, 750, 58, 36, 20, 124, 34, 12, 142],\n",
       " [62, 740],\n",
       " [62, 740, 1],\n",
       " [62, 740, 1, 467],\n",
       " [62, 740, 1, 467, 967],\n",
       " [62, 740, 1, 467, 967, 7],\n",
       " [62, 740, 1, 467, 967, 7, 1],\n",
       " [62, 740, 1, 467, 967, 7, 1, 966],\n",
       " [62, 740, 1, 467, 967, 7, 1, 966, 401],\n",
       " [62, 740, 1, 467, 967, 7, 1, 966, 401, 1368],\n",
       " [62, 740, 1, 467, 967, 7, 1, 966, 401, 1368, 3],\n",
       " [15, 50],\n",
       " [15, 50, 751],\n",
       " [15, 50, 751, 57],\n",
       " [15, 50, 751, 57, 58],\n",
       " [15, 50, 751, 57, 58, 33],\n",
       " [15, 50, 751, 57, 58, 33, 4],\n",
       " [15, 50, 751, 57, 58, 33, 4, 1369],\n",
       " [15, 50, 751, 57, 58, 33, 4, 1369, 1370],\n",
       " [15, 50, 751, 57, 58, 33, 4, 1369, 1370, 21],\n",
       " [15, 50, 751, 57, 58, 33, 4, 1369, 1370, 21, 11],\n",
       " [19, 30],\n",
       " [19, 30, 752],\n",
       " [19, 30, 752, 1371],\n",
       " [19, 30, 752, 1371, 3],\n",
       " [19, 30, 752, 1371, 3, 753],\n",
       " [19, 30, 752, 1371, 3, 753, 13],\n",
       " [19, 30, 752, 1371, 3, 753, 13, 10],\n",
       " [19, 30, 752, 1371, 3, 753, 13, 10, 968],\n",
       " [19, 30, 752, 1371, 3, 753, 13, 10, 968, 399],\n",
       " [19, 30, 752, 1371, 3, 753, 13, 10, 968, 399, 6],\n",
       " [969, 14],\n",
       " [969, 14, 9],\n",
       " [969, 14, 9, 152],\n",
       " [969, 14, 9, 152, 181],\n",
       " [969, 14, 9, 152, 181, 754],\n",
       " [969, 14, 9, 152, 181, 754, 35],\n",
       " [969, 14, 9, 152, 181, 754, 35, 402],\n",
       " [969, 14, 9, 152, 181, 754, 35, 402, 5],\n",
       " [969, 14, 9, 152, 181, 754, 35, 402, 5, 319],\n",
       " [969, 14, 9, 152, 181, 754, 35, 402, 5, 319, 19],\n",
       " [245, 102],\n",
       " [245, 102, 128],\n",
       " [245, 102, 128, 15],\n",
       " [245, 102, 128, 15, 1],\n",
       " [245, 102, 128, 15, 1, 468],\n",
       " [245, 102, 128, 15, 1, 468, 28],\n",
       " [245, 102, 128, 15, 1, 468, 28, 1],\n",
       " [245, 102, 128, 15, 1, 468, 28, 1, 1372],\n",
       " [245, 102, 128, 15, 1, 468, 28, 1, 1372, 1373],\n",
       " [755, 970],\n",
       " [755, 970, 5],\n",
       " [755, 970, 5, 127],\n",
       " [755, 970, 5, 127, 6],\n",
       " [755, 970, 5, 127, 6, 357],\n",
       " [755, 970, 5, 127, 6, 357, 12],\n",
       " [755, 970, 5, 127, 6, 357, 12, 1374],\n",
       " [755, 970, 5, 127, 6, 357, 12, 1374, 971],\n",
       " [755, 970, 5, 127, 6, 357, 12, 1374, 971, 3],\n",
       " [755, 970, 5, 127, 6, 357, 12, 1374, 971, 3, 1375],\n",
       " [755, 970, 5, 127, 6, 357, 12, 1374, 971, 3, 1375, 1],\n",
       " [1376, 1377],\n",
       " [1376, 1377, 20],\n",
       " [1376, 1377, 20, 2],\n",
       " [1376, 1377, 20, 2, 135],\n",
       " [1376, 1377, 20, 2, 135, 70],\n",
       " [1376, 1377, 20, 2, 135, 70, 1378],\n",
       " [1376, 1377, 20, 2, 135, 70, 1378, 13],\n",
       " [1376, 1377, 20, 2, 135, 70, 1378, 13, 1],\n",
       " [1376, 1377, 20, 2, 135, 70, 1378, 13, 1, 468],\n",
       " [1376, 1377, 20, 2, 135, 70, 1378, 13, 1, 468, 1379],\n",
       " [11, 50],\n",
       " [11, 50, 53],\n",
       " [11, 50, 53, 7],\n",
       " [11, 50, 53, 7, 258],\n",
       " [11, 50, 53, 7, 258, 402],\n",
       " [11, 50, 53, 7, 258, 402, 21],\n",
       " [11, 50, 53, 7, 258, 402, 21, 11],\n",
       " [11, 50, 53, 7, 258, 402, 21, 11, 510],\n",
       " [11, 50, 53, 7, 258, 402, 21, 11, 510, 56],\n",
       " [11, 50, 53, 7, 258, 402, 21, 11, 510, 56, 1380],\n",
       " [11, 50, 53, 7, 258, 402, 21, 11, 510, 56, 1380, 3],\n",
       " [11, 50, 53, 7, 258, 402, 21, 11, 510, 56, 1380, 3, 295],\n",
       " [52, 19],\n",
       " [52, 19, 4],\n",
       " [52, 19, 4, 32],\n",
       " [52, 19, 4, 32, 35],\n",
       " [52, 19, 4, 32, 35, 615],\n",
       " [52, 19, 4, 32, 35, 615, 469],\n",
       " [52, 19, 4, 32, 35, 615, 469, 10],\n",
       " [52, 19, 4, 32, 35, 615, 469, 10, 40],\n",
       " [52, 19, 4, 32, 35, 615, 469, 10, 40, 1381],\n",
       " [52, 19, 4, 32, 35, 615, 469, 10, 40, 1381, 14],\n",
       " [52, 19, 4, 32, 35, 615, 469, 10, 40, 1381, 14, 51],\n",
       " [52, 19, 4, 32, 35, 615, 469, 10, 40, 1381, 14, 51, 10],\n",
       " [524, 756],\n",
       " [524, 756, 28],\n",
       " [524, 756, 28, 1382],\n",
       " [524, 756, 28, 1382, 1383],\n",
       " [524, 756, 28, 1382, 1383, 88],\n",
       " [524, 756, 28, 1382, 1383, 88, 451],\n",
       " [524, 756, 28, 1382, 1383, 88, 451, 23],\n",
       " [524, 756, 28, 1382, 1383, 88, 451, 23, 37],\n",
       " [524, 756, 28, 1382, 1383, 88, 451, 23, 37, 142],\n",
       " [166, 327],\n",
       " [166, 327, 99],\n",
       " [166, 327, 99, 3],\n",
       " [166, 327, 99, 3, 48],\n",
       " [166, 327, 99, 3, 48, 214],\n",
       " [166, 327, 99, 3, 48, 214, 87],\n",
       " [166, 327, 99, 3, 48, 214, 87, 972],\n",
       " [166, 327, 99, 3, 48, 214, 87, 972, 65],\n",
       " [166, 327, 99, 3, 48, 214, 87, 972, 65, 525],\n",
       " [4, 7],\n",
       " [4, 7, 74],\n",
       " [4, 7, 74, 403],\n",
       " [4, 7, 74, 403, 1384],\n",
       " [4, 7, 74, 403, 1384, 26],\n",
       " [4, 7, 74, 403, 1384, 26, 40],\n",
       " [4, 7, 74, 403, 1384, 26, 40, 156],\n",
       " [4, 7, 74, 403, 1384, 26, 40, 156, 450],\n",
       " [4, 7, 74, 403, 1384, 26, 40, 156, 450, 2],\n",
       " [4, 7, 74, 403, 1384, 26, 40, 156, 450, 2, 1385],\n",
       " [2, 72],\n",
       " [2, 72, 31],\n",
       " [2, 72, 31, 6],\n",
       " [2, 72, 31, 6, 1],\n",
       " [2, 72, 31, 6, 1, 112],\n",
       " [2, 72, 31, 6, 1, 112, 55],\n",
       " [2, 72, 31, 6, 1, 112, 55, 1],\n",
       " [2, 72, 31, 6, 1, 112, 55, 1, 521],\n",
       " [2, 72, 31, 6, 1, 112, 55, 1, 521, 470],\n",
       " [2, 72, 31, 6, 1, 112, 55, 1, 521, 470, 465],\n",
       " [2, 72, 31, 6, 1, 112, 55, 1, 521, 470, 465, 2],\n",
       " [2, 72, 31, 6, 1, 112, 55, 1, 521, 470, 465, 2, 326],\n",
       " [34, 7],\n",
       " [34, 7, 12],\n",
       " [34, 7, 12, 973],\n",
       " [34, 7, 12, 973, 757],\n",
       " [34, 7, 12, 973, 757, 22],\n",
       " [34, 7, 12, 973, 757, 22, 1],\n",
       " [34, 7, 12, 973, 757, 22, 1, 43],\n",
       " [34, 7, 12, 973, 757, 22, 1, 43, 404],\n",
       " [34, 7, 12, 973, 757, 22, 1, 43, 404, 5],\n",
       " [34, 7, 12, 973, 757, 22, 1, 43, 404, 5, 85],\n",
       " [34, 7, 12, 973, 757, 22, 1, 43, 404, 5, 85, 14],\n",
       " [616, 233],\n",
       " [616, 233, 8],\n",
       " [616, 233, 8, 7],\n",
       " [616, 233, 8, 7, 5],\n",
       " [616, 233, 8, 7, 5, 46],\n",
       " [616, 233, 8, 7, 5, 46, 203],\n",
       " [616, 233, 8, 7, 5, 46, 203, 465],\n",
       " [616, 233, 8, 7, 5, 46, 203, 465, 2],\n",
       " [616, 233, 8, 7, 5, 46, 203, 465, 2, 326],\n",
       " [273, 66],\n",
       " [273, 66, 278],\n",
       " [273, 66, 278, 2],\n",
       " [273, 66, 278, 2, 78],\n",
       " [273, 66, 278, 2, 78, 617],\n",
       " [273, 66, 278, 2, 78, 617, 11],\n",
       " [273, 66, 278, 2, 78, 617, 11, 1386],\n",
       " [273, 66, 278, 2, 78, 617, 11, 1386, 196],\n",
       " [273, 66, 278, 2, 78, 617, 11, 1386, 196, 14],\n",
       " [273, 66, 278, 2, 78, 617, 11, 1386, 196, 14, 7],\n",
       " [526, 55],\n",
       " [526, 55, 4],\n",
       " [526, 55, 4, 165],\n",
       " [526, 55, 4, 165, 5],\n",
       " [526, 55, 4, 165, 5, 299],\n",
       " [526, 55, 4, 165, 5, 299, 196],\n",
       " [526, 55, 4, 165, 5, 299, 196, 1387],\n",
       " [526, 55, 4, 165, 5, 299, 196, 1387, 8],\n",
       " [526, 55, 4, 165, 5, 299, 196, 1387, 8, 29],\n",
       " [526, 55, 4, 165, 5, 299, 196, 1387, 8, 29, 201],\n",
       " [526, 55, 4, 165, 5, 299, 196, 1387, 8, 29, 201, 30],\n",
       " [1388, 13],\n",
       " [1388, 13, 1],\n",
       " ...]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now applying pad sequencing to make every sentence is of the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sequence_len=max([len(x) for x in input_sequences])\n",
    "max_sequence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,    0, 1313,  139],\n",
       "       [   0,    0,    0, ..., 1313,  139,   55],\n",
       "       [   0,    0,    0, ...,  139,   55,   14],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  770,   31,   18],\n",
       "       [   0,    0,    0, ...,   31,   18,  930],\n",
       "       [   0,    0,    0, ...,   18,  930,  131]], dtype=int32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequences=np.array(pad_sequences(input_sequences,maxlen=max_sequence_len,padding='pre'))\n",
    "input_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating predictors and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = input_sequences[:,:-1], input_sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,    0,    0, 1313],\n",
       "       [   0,    0,    0, ...,    0, 1313,  139],\n",
       "       [   0,    0,    0, ..., 1313,  139,   55],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,    3,  770,   31],\n",
       "       [   0,    0,    0, ...,  770,   31,   18],\n",
       "       [   0,    0,    0, ...,   31,   18,  930]], dtype=int32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([139,  55,  14, ...,  18, 930, 131], dtype=int32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.keras.utils.to_categorical(y,num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the data into -> Train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_10\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_10\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">261,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">150,600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">100,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2618</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">264,418</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_10 (\u001b[38;5;33mEmbedding\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │       \u001b[38;5;34m261,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_20 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m150\u001b[0m)       │       \u001b[38;5;34m150,600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m150\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_21 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │       \u001b[38;5;34m100,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2618\u001b[0m)           │       \u001b[38;5;34m264,418\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">777,218</span> (2.96 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m777,218\u001b[0m (2.96 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">777,218</span> (2.96 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m777,218\u001b[0m (2.96 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Train our LSTM RNN\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout,GRU\n",
    "\n",
    "# Parameters\n",
    "vocab_size = 10000  # example vocabulary size\n",
    "embedding_dim = 128  # example embedding dimension\n",
    "input_length = 100  # example input length (sequence length)\n",
    "lstm_units = 150  # example LSTM units\n",
    "\n",
    "## Define the model\n",
    "model=Sequential()\n",
    "model.add(Embedding(total_words,100,input_length=max_sequence_len-1))\n",
    "model.add(LSTM(150,return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(total_words,activation=\"softmax\"))\n",
    "\n",
    "# #Compile the model\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "model.build((None , input_length))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.3523 - loss: 2.7960 - val_accuracy: 0.1183 - val_loss: 8.2472\n",
      "Epoch 2/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - accuracy: 0.3601 - loss: 2.7524 - val_accuracy: 0.1171 - val_loss: 8.2696\n",
      "Epoch 3/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - accuracy: 0.3645 - loss: 2.7393 - val_accuracy: 0.1185 - val_loss: 8.3462\n",
      "Epoch 4/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - accuracy: 0.3698 - loss: 2.7029 - val_accuracy: 0.1203 - val_loss: 8.3770\n",
      "Epoch 5/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.3697 - loss: 2.6882 - val_accuracy: 0.1190 - val_loss: 8.4165\n",
      "Epoch 6/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.3738 - loss: 2.6818 - val_accuracy: 0.1141 - val_loss: 8.4567\n",
      "Epoch 7/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.3728 - loss: 2.6543 - val_accuracy: 0.1136 - val_loss: 8.4865\n",
      "Epoch 8/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.3765 - loss: 2.6285 - val_accuracy: 0.1151 - val_loss: 8.5464\n",
      "Epoch 9/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - accuracy: 0.3879 - loss: 2.5944 - val_accuracy: 0.1138 - val_loss: 8.5771\n",
      "Epoch 10/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - accuracy: 0.3906 - loss: 2.5901 - val_accuracy: 0.1119 - val_loss: 8.6121\n",
      "Epoch 11/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - accuracy: 0.3967 - loss: 2.5834 - val_accuracy: 0.1173 - val_loss: 8.6646\n",
      "Epoch 12/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - accuracy: 0.3983 - loss: 2.5466 - val_accuracy: 0.1158 - val_loss: 8.6985\n",
      "Epoch 13/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.4075 - loss: 2.5142 - val_accuracy: 0.1104 - val_loss: 8.7245\n",
      "Epoch 14/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.4078 - loss: 2.5067 - val_accuracy: 0.1178 - val_loss: 8.7667\n",
      "Epoch 15/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - accuracy: 0.4122 - loss: 2.4794 - val_accuracy: 0.1119 - val_loss: 8.8083\n",
      "Epoch 16/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.4074 - loss: 2.4948 - val_accuracy: 0.1123 - val_loss: 8.8526\n",
      "Epoch 17/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.4201 - loss: 2.4584 - val_accuracy: 0.1086 - val_loss: 8.8779\n",
      "Epoch 18/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.4236 - loss: 2.4221 - val_accuracy: 0.1123 - val_loss: 8.9013\n",
      "Epoch 19/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 41ms/step - accuracy: 0.4272 - loss: 2.4067 - val_accuracy: 0.1091 - val_loss: 8.9415\n",
      "Epoch 20/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.4362 - loss: 2.3850 - val_accuracy: 0.1091 - val_loss: 9.0024\n",
      "Epoch 21/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.4349 - loss: 2.3766 - val_accuracy: 0.1079 - val_loss: 9.0059\n",
      "Epoch 22/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.4397 - loss: 2.3634 - val_accuracy: 0.1128 - val_loss: 9.0602\n",
      "Epoch 23/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.4483 - loss: 2.3187 - val_accuracy: 0.1106 - val_loss: 9.0968\n",
      "Epoch 24/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.4404 - loss: 2.3318 - val_accuracy: 0.1091 - val_loss: 9.1360\n",
      "Epoch 25/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.4593 - loss: 2.2833 - val_accuracy: 0.1099 - val_loss: 9.1684\n",
      "Epoch 26/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.4556 - loss: 2.2728 - val_accuracy: 0.1067 - val_loss: 9.2134\n",
      "Epoch 27/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.4502 - loss: 2.2853 - val_accuracy: 0.1079 - val_loss: 9.2311\n",
      "Epoch 28/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.4602 - loss: 2.2448 - val_accuracy: 0.1054 - val_loss: 9.2617\n",
      "Epoch 29/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.4639 - loss: 2.2283 - val_accuracy: 0.1101 - val_loss: 9.3121\n",
      "Epoch 30/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.4668 - loss: 2.1977 - val_accuracy: 0.1037 - val_loss: 9.3290\n",
      "Epoch 31/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.4688 - loss: 2.2009 - val_accuracy: 0.1042 - val_loss: 9.3736\n",
      "Epoch 32/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.4755 - loss: 2.1708 - val_accuracy: 0.1067 - val_loss: 9.4309\n",
      "Epoch 33/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.4783 - loss: 2.1433 - val_accuracy: 0.1091 - val_loss: 9.4416\n",
      "Epoch 34/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.4825 - loss: 2.1361 - val_accuracy: 0.1079 - val_loss: 9.4675\n",
      "Epoch 35/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - accuracy: 0.4826 - loss: 2.1376 - val_accuracy: 0.1079 - val_loss: 9.4885\n",
      "Epoch 36/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.4811 - loss: 2.1286 - val_accuracy: 0.1067 - val_loss: 9.5420\n",
      "Epoch 37/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.4973 - loss: 2.0965 - val_accuracy: 0.1074 - val_loss: 9.5754\n",
      "Epoch 38/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.4995 - loss: 2.0688 - val_accuracy: 0.1064 - val_loss: 9.6289\n",
      "Epoch 39/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.4998 - loss: 2.0690 - val_accuracy: 0.1064 - val_loss: 9.6362\n",
      "Epoch 40/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.4999 - loss: 2.0403 - val_accuracy: 0.1052 - val_loss: 9.6923\n",
      "Epoch 41/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.5087 - loss: 2.0276 - val_accuracy: 0.1054 - val_loss: 9.7221\n",
      "Epoch 42/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.5140 - loss: 2.0124 - val_accuracy: 0.1067 - val_loss: 9.7398\n",
      "Epoch 43/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.5104 - loss: 1.9963 - val_accuracy: 0.1059 - val_loss: 9.7818\n",
      "Epoch 44/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.5160 - loss: 1.9981 - val_accuracy: 0.1047 - val_loss: 9.7916\n",
      "Epoch 45/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.5259 - loss: 1.9372 - val_accuracy: 0.1047 - val_loss: 9.8453\n",
      "Epoch 46/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.5244 - loss: 1.9523 - val_accuracy: 0.0995 - val_loss: 9.8751\n",
      "Epoch 47/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - accuracy: 0.5299 - loss: 1.9321 - val_accuracy: 0.1012 - val_loss: 9.8989\n",
      "Epoch 48/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - accuracy: 0.5272 - loss: 1.9248 - val_accuracy: 0.1020 - val_loss: 9.9463\n",
      "Epoch 49/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.5279 - loss: 1.9171 - val_accuracy: 0.1037 - val_loss: 9.9657\n",
      "Epoch 50/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.5437 - loss: 1.8782 - val_accuracy: 0.1032 - val_loss: 9.9980\n",
      "Epoch 51/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.5431 - loss: 1.8597 - val_accuracy: 0.1015 - val_loss: 10.0219\n",
      "Epoch 52/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - accuracy: 0.5392 - loss: 1.8627 - val_accuracy: 0.1057 - val_loss: 10.0541\n",
      "Epoch 53/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.5540 - loss: 1.8387 - val_accuracy: 0.1012 - val_loss: 10.1095\n",
      "Epoch 54/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.5507 - loss: 1.8402 - val_accuracy: 0.1002 - val_loss: 10.1230\n",
      "Epoch 55/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.5587 - loss: 1.7936 - val_accuracy: 0.0987 - val_loss: 10.1532\n",
      "Epoch 56/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.5630 - loss: 1.8017 - val_accuracy: 0.1002 - val_loss: 10.1832\n",
      "Epoch 57/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.5651 - loss: 1.7824 - val_accuracy: 0.0992 - val_loss: 10.2152\n",
      "Epoch 58/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.5699 - loss: 1.7591 - val_accuracy: 0.1015 - val_loss: 10.2536\n",
      "Epoch 59/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.5675 - loss: 1.7521 - val_accuracy: 0.1007 - val_loss: 10.2664\n",
      "Epoch 60/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.5723 - loss: 1.7457 - val_accuracy: 0.0990 - val_loss: 10.3097\n",
      "Epoch 61/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.5817 - loss: 1.7224 - val_accuracy: 0.0997 - val_loss: 10.3440\n",
      "Epoch 62/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.5839 - loss: 1.7013 - val_accuracy: 0.1002 - val_loss: 10.3843\n",
      "Epoch 63/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.5925 - loss: 1.6685 - val_accuracy: 0.1024 - val_loss: 10.3870\n",
      "Epoch 64/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - accuracy: 0.5872 - loss: 1.6810 - val_accuracy: 0.0997 - val_loss: 10.4191\n",
      "Epoch 65/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.5923 - loss: 1.6557 - val_accuracy: 0.0968 - val_loss: 10.4580\n",
      "Epoch 66/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.5881 - loss: 1.6690 - val_accuracy: 0.0955 - val_loss: 10.5037\n",
      "Epoch 67/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.5958 - loss: 1.6482 - val_accuracy: 0.0968 - val_loss: 10.4965\n",
      "Epoch 68/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.6001 - loss: 1.6248 - val_accuracy: 0.1005 - val_loss: 10.5538\n",
      "Epoch 69/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.6031 - loss: 1.6192 - val_accuracy: 0.0982 - val_loss: 10.5550\n",
      "Epoch 70/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.6110 - loss: 1.5923 - val_accuracy: 0.0985 - val_loss: 10.5942\n",
      "Epoch 71/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - accuracy: 0.6095 - loss: 1.5976 - val_accuracy: 0.0975 - val_loss: 10.6560\n",
      "Epoch 72/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.6132 - loss: 1.5677 - val_accuracy: 0.0982 - val_loss: 10.6649\n",
      "Epoch 73/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.6137 - loss: 1.5583 - val_accuracy: 0.0997 - val_loss: 10.7301\n",
      "Epoch 74/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - accuracy: 0.6241 - loss: 1.5445 - val_accuracy: 0.0982 - val_loss: 10.7322\n",
      "Epoch 75/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.6245 - loss: 1.5258 - val_accuracy: 0.0940 - val_loss: 10.7710\n",
      "Epoch 76/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - accuracy: 0.6298 - loss: 1.5218 - val_accuracy: 0.0916 - val_loss: 10.7880\n",
      "Epoch 77/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.6393 - loss: 1.4924 - val_accuracy: 0.0926 - val_loss: 10.8105\n",
      "Epoch 78/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.6351 - loss: 1.4934 - val_accuracy: 0.0980 - val_loss: 10.8324\n",
      "Epoch 79/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.6406 - loss: 1.4758 - val_accuracy: 0.0916 - val_loss: 10.8597\n",
      "Epoch 80/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.6349 - loss: 1.4761 - val_accuracy: 0.0973 - val_loss: 10.8843\n",
      "Epoch 81/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.6448 - loss: 1.4626 - val_accuracy: 0.0943 - val_loss: 10.9660\n",
      "Epoch 82/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - accuracy: 0.6464 - loss: 1.4425 - val_accuracy: 0.0906 - val_loss: 10.9724\n",
      "Epoch 83/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.6439 - loss: 1.4469 - val_accuracy: 0.0940 - val_loss: 10.9691\n",
      "Epoch 84/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - accuracy: 0.6515 - loss: 1.4218 - val_accuracy: 0.0911 - val_loss: 10.9937\n",
      "Epoch 85/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.6542 - loss: 1.4079 - val_accuracy: 0.0921 - val_loss: 11.0308\n",
      "Epoch 86/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - accuracy: 0.6608 - loss: 1.3943 - val_accuracy: 0.0950 - val_loss: 11.0623\n",
      "Epoch 87/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.6636 - loss: 1.3717 - val_accuracy: 0.0921 - val_loss: 11.0996\n",
      "Epoch 88/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.6627 - loss: 1.3687 - val_accuracy: 0.0916 - val_loss: 11.1268\n",
      "Epoch 89/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.6662 - loss: 1.3536 - val_accuracy: 0.0908 - val_loss: 11.1594\n",
      "Epoch 90/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.6680 - loss: 1.3548 - val_accuracy: 0.0903 - val_loss: 11.1607\n",
      "Epoch 91/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.6725 - loss: 1.3244 - val_accuracy: 0.0888 - val_loss: 11.1886\n",
      "Epoch 92/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - accuracy: 0.6752 - loss: 1.3364 - val_accuracy: 0.0864 - val_loss: 11.2365\n",
      "Epoch 93/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - accuracy: 0.6717 - loss: 1.3311 - val_accuracy: 0.0950 - val_loss: 11.2735\n",
      "Epoch 94/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.6845 - loss: 1.2930 - val_accuracy: 0.0883 - val_loss: 11.2847\n",
      "Epoch 95/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.6773 - loss: 1.2995 - val_accuracy: 0.0916 - val_loss: 11.3087\n",
      "Epoch 96/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 41ms/step - accuracy: 0.6820 - loss: 1.2930 - val_accuracy: 0.0908 - val_loss: 11.3331\n",
      "Epoch 97/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.6922 - loss: 1.2603 - val_accuracy: 0.0883 - val_loss: 11.3758\n",
      "Epoch 98/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - accuracy: 0.6938 - loss: 1.2458 - val_accuracy: 0.0891 - val_loss: 11.3948\n",
      "Epoch 99/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 41ms/step - accuracy: 0.6879 - loss: 1.2719 - val_accuracy: 0.0901 - val_loss: 11.4271\n",
      "Epoch 100/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 41ms/step - accuracy: 0.7015 - loss: 1.2447 - val_accuracy: 0.0891 - val_loss: 11.4455\n",
      "Epoch 101/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.7000 - loss: 1.2418 - val_accuracy: 0.0913 - val_loss: 11.5031\n",
      "Epoch 102/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - accuracy: 0.6973 - loss: 1.2277 - val_accuracy: 0.0923 - val_loss: 11.5017\n",
      "Epoch 103/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - accuracy: 0.7007 - loss: 1.2181 - val_accuracy: 0.0901 - val_loss: 11.5042\n",
      "Epoch 104/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.7017 - loss: 1.1986 - val_accuracy: 0.0906 - val_loss: 11.5254\n",
      "Epoch 105/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.7146 - loss: 1.1762 - val_accuracy: 0.0898 - val_loss: 11.5780\n",
      "Epoch 106/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 41ms/step - accuracy: 0.7130 - loss: 1.1665 - val_accuracy: 0.0888 - val_loss: 11.5901\n",
      "Epoch 107/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.7125 - loss: 1.1815 - val_accuracy: 0.0883 - val_loss: 11.6337\n",
      "Epoch 108/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - accuracy: 0.7172 - loss: 1.1621 - val_accuracy: 0.0903 - val_loss: 11.6507\n",
      "Epoch 109/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - accuracy: 0.7112 - loss: 1.1738 - val_accuracy: 0.0916 - val_loss: 11.6580\n",
      "Epoch 110/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.7255 - loss: 1.1342 - val_accuracy: 0.0859 - val_loss: 11.6602\n",
      "Epoch 111/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - accuracy: 0.7285 - loss: 1.1147 - val_accuracy: 0.0876 - val_loss: 11.7134\n",
      "Epoch 112/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.7259 - loss: 1.1206 - val_accuracy: 0.0911 - val_loss: 11.7502\n",
      "Epoch 113/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.7328 - loss: 1.1092 - val_accuracy: 0.0913 - val_loss: 11.7529\n",
      "Epoch 114/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - accuracy: 0.7249 - loss: 1.1237 - val_accuracy: 0.0918 - val_loss: 11.7869\n",
      "Epoch 115/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - accuracy: 0.7351 - loss: 1.0826 - val_accuracy: 0.0903 - val_loss: 11.8144\n",
      "Epoch 116/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - accuracy: 0.7388 - loss: 1.0787 - val_accuracy: 0.0906 - val_loss: 11.8353\n",
      "Epoch 117/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.7362 - loss: 1.0827 - val_accuracy: 0.0866 - val_loss: 11.8949\n",
      "Epoch 118/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - accuracy: 0.7389 - loss: 1.0665 - val_accuracy: 0.0898 - val_loss: 11.8803\n",
      "Epoch 119/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.7401 - loss: 1.0761 - val_accuracy: 0.0906 - val_loss: 11.8938\n",
      "Epoch 120/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - accuracy: 0.7461 - loss: 1.0419 - val_accuracy: 0.0881 - val_loss: 11.9485\n",
      "Epoch 121/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - accuracy: 0.7466 - loss: 1.0440 - val_accuracy: 0.0878 - val_loss: 11.9568\n",
      "Epoch 122/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.7497 - loss: 1.0233 - val_accuracy: 0.0856 - val_loss: 11.9638\n",
      "Epoch 123/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.7448 - loss: 1.0403 - val_accuracy: 0.0876 - val_loss: 12.0121\n",
      "Epoch 124/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.7548 - loss: 1.0149 - val_accuracy: 0.0876 - val_loss: 12.0064\n",
      "Epoch 125/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 42ms/step - accuracy: 0.7495 - loss: 1.0112 - val_accuracy: 0.0896 - val_loss: 12.0435\n",
      "Epoch 126/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.7554 - loss: 0.9995 - val_accuracy: 0.0856 - val_loss: 12.0844\n",
      "Epoch 127/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.7624 - loss: 0.9844 - val_accuracy: 0.0888 - val_loss: 12.0853\n",
      "Epoch 128/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.7575 - loss: 0.9885 - val_accuracy: 0.0856 - val_loss: 12.1344\n",
      "Epoch 129/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.7553 - loss: 0.9907 - val_accuracy: 0.0874 - val_loss: 12.1468\n",
      "Epoch 130/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 693ms/step - accuracy: 0.7664 - loss: 0.9585 - val_accuracy: 0.0866 - val_loss: 12.1327\n",
      "Epoch 131/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 1s/step - accuracy: 0.7638 - loss: 0.9692 - val_accuracy: 0.0878 - val_loss: 12.1677\n",
      "Epoch 132/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 105ms/step - accuracy: 0.7669 - loss: 0.9577 - val_accuracy: 0.0864 - val_loss: 12.2416\n",
      "Epoch 133/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 827ms/step - accuracy: 0.7679 - loss: 0.9483 - val_accuracy: 0.0831 - val_loss: 12.2356\n",
      "Epoch 134/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 273ms/step - accuracy: 0.7750 - loss: 0.9356 - val_accuracy: 0.0849 - val_loss: 12.2826\n",
      "Epoch 135/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 574ms/step - accuracy: 0.7759 - loss: 0.9318 - val_accuracy: 0.0869 - val_loss: 12.2823\n",
      "Epoch 136/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m816s\u001b[0m 3s/step - accuracy: 0.7736 - loss: 0.9298 - val_accuracy: 0.0866 - val_loss: 12.3324\n",
      "Epoch 137/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 296ms/step - accuracy: 0.7758 - loss: 0.9206 - val_accuracy: 0.0869 - val_loss: 12.3420\n",
      "Epoch 138/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 497ms/step - accuracy: 0.7788 - loss: 0.9090 - val_accuracy: 0.0866 - val_loss: 12.3801\n",
      "Epoch 139/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m936s\u001b[0m 4s/step - accuracy: 0.7836 - loss: 0.8935 - val_accuracy: 0.0849 - val_loss: 12.4013\n",
      "Epoch 140/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 140ms/step - accuracy: 0.7886 - loss: 0.8850 - val_accuracy: 0.0814 - val_loss: 12.3877\n",
      "Epoch 141/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 49ms/step - accuracy: 0.7815 - loss: 0.9027 - val_accuracy: 0.0854 - val_loss: 12.4375\n",
      "Epoch 142/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.7866 - loss: 0.8832 - val_accuracy: 0.0846 - val_loss: 12.4431\n",
      "Epoch 143/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - accuracy: 0.7896 - loss: 0.8700 - val_accuracy: 0.0844 - val_loss: 12.5029\n",
      "Epoch 144/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - accuracy: 0.7947 - loss: 0.8471 - val_accuracy: 0.0839 - val_loss: 12.4792\n",
      "Epoch 145/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 198ms/step - accuracy: 0.7900 - loss: 0.8684 - val_accuracy: 0.0878 - val_loss: 12.5382\n",
      "Epoch 146/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.7922 - loss: 0.8610 - val_accuracy: 0.0874 - val_loss: 12.5001\n",
      "Epoch 147/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - accuracy: 0.8014 - loss: 0.8293 - val_accuracy: 0.0844 - val_loss: 12.5351\n",
      "Epoch 148/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - accuracy: 0.7963 - loss: 0.8373 - val_accuracy: 0.0841 - val_loss: 12.5740\n",
      "Epoch 149/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - accuracy: 0.8036 - loss: 0.8217 - val_accuracy: 0.0859 - val_loss: 12.5964\n",
      "Epoch 150/150\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.7948 - loss: 0.8415 - val_accuracy: 0.0856 - val_loss: 12.5986\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(x_train,y_train, epochs=150, batch_size=64,validation_data=(x_test,y_test),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(model, tokenizer, text, max_sequence_len):\n",
    "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
    "    if len(token_list) >= max_sequence_len:\n",
    "        token_list = token_list[-(max_sequence_len-1):]  # Ensure the sequence length matches max_sequence_len-1\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = model.predict(token_list, verbose=0)\n",
    "    predicted_word_index = np.argmax(predicted, axis=1)\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted_word_index:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:Can we go in that \n",
      "Next Word Prediction: way\n"
     ]
    }
   ],
   "source": [
    "input_text=\"Can we go in that \"\n",
    "print(f\"Input text:{input_text}\")\n",
    "max_sequence_len=model.input_shape[1]+1\n",
    "next_word=predict_next_word(model,tokenizer,input_text,max_sequence_len)\n",
    "print(f\"Next Word Prediction: {next_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "## Save the model\n",
    "model.save(\"next_word_lstm.h5\")\n",
    "## Save the tokenizer\n",
    "import pickle\n",
    "with open('tokenizer.pickle','wb') as handle:\n",
    "    pickle.dump(tokenizer,handle,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
